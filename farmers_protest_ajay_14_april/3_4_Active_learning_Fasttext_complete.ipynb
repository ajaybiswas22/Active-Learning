{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\AJAY\n",
      "[nltk_data]     BISWAS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import fasttext\n",
    "import pandas as pd\n",
    "import random\n",
    "import itertools\n",
    "from resources.tokTT import CommentTokenizer as CT\n",
    "from resources.basicIO import InputOutput as IO\n",
    "from resources.basicIO import InputOutput as IO\n",
    "from resources.filterLang import FilterLanguage as FL\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import fasttext\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "from scipy.spatial import distance\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "#from sklearn.linear_model import LassoLars\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import copy\n",
    "import scipy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Corpus and Random Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_corpus = IO.load_text('datasets/corpus.txt')\n",
    "tokenized_corpus = CT.cleaned('datasets/corpus.txt')\n",
    "IO.save_text('datasets/tokenized_corpus.txt',tokenized_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Fasttext Unsupervised Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = fasttext.train_unsupervised(\n",
    "    input=\"datasets/tokenized_corpus.txt\", lr=0.01, epoch=40, wordNgrams=2, dim=300)\n",
    "model_2.save_model(\"models/ft_unsupervised_N_2.bin\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus\n",
    "df_dict = {'raw_comment': raw_corpus, 'tokenized_comment': tokenized_corpus}\n",
    "df_corpus = pd.DataFrame(df_dict)\n",
    "df_corpus.to_csv('datasets/corpus_data.csv', index=False)\n",
    "\n",
    "# random sample\n",
    "text = IO.load_csv_col('datasets/random_sample.csv', 'comment')\n",
    "text_labels = IO.load_csv_col('datasets/random_sample.csv', 'label')\n",
    "text_labels = list(map(str, map(int, text_labels)))\n",
    "text_TK = [CT.tokenize(x) for x in text]\n",
    "\n",
    "df_dict = {'raw_comment': text,\n",
    "           'tokenized_comment': text_TK, 'label': text_labels}\n",
    "df_sample = pd.DataFrame(df_dict)\n",
    "df_sample.to_csv('datasets/random_sample_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Unnecessary Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unnecessary comments\n",
    "df_sample = df_sample.drop([x for x in range(len(df_sample)) if int(\n",
    "    df_sample['label'][x]) != 0 and int(df_sample['label'][x]) != 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_comment</th>\n",
       "      <th>tokenized_comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3448</th>\n",
       "      <td>These new agri. laws are not feasible in india...</td>\n",
       "      <td>these new agri law be not feasible in india fi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3450</th>\n",
       "      <td>If this is Farmer protest , why we see only pu...</td>\n",
       "      <td>if this be farmer protest why we see only punj...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3451</th>\n",
       "      <td>UP police is trying to clear their image in th...</td>\n",
       "      <td>up police be try to clear their image in the w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3452</th>\n",
       "      <td>Support farmers</td>\n",
       "      <td>support farmer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3453</th>\n",
       "      <td>I agree to many points you made, but MSP is a ...</td>\n",
       "      <td>i agree to many point you make but msp be a do...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            raw_comment  \\\n",
       "3448  These new agri. laws are not feasible in india...   \n",
       "3450  If this is Farmer protest , why we see only pu...   \n",
       "3451  UP police is trying to clear their image in th...   \n",
       "3452                                   Support farmers    \n",
       "3453  I agree to many points you made, but MSP is a ...   \n",
       "\n",
       "                                      tokenized_comment label  \n",
       "3448  these new agri law be not feasible in india fi...     0  \n",
       "3450  if this be farmer protest why we see only punj...     1  \n",
       "3451  up police be try to clear their image in the w...     0  \n",
       "3452                                     support farmer     0  \n",
       "3453  i agree to many point you make but msp be a do...     1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1408\n",
       "1    1074\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample['label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_sample['tokenized_comment'],\n",
    "                                                    df_sample['label'], test_size=0.2,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=df_sample['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  1985\n",
      "X_test:  497\n"
     ]
    }
   ],
   "source": [
    "print('X_train: ' ,len(X_train))\n",
    "print('X_test: ' ,len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastTextTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Convert texts into their mean fastText vectors \"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.stack([np.mean([self.model[w] for w in text.split()], 0) for text in X])\n",
    "\n",
    "\n",
    "def classify(small_model, predictor, lines, Y):\n",
    "    classifier = make_pipeline(\n",
    "        FastTextTransformer(model=small_model),\n",
    "        predictor\n",
    "    ).fit(\n",
    "        lines,\n",
    "        Y\n",
    "    )\n",
    "    return classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "LR_Normal = classify(model_2, LogisticRegression(\n",
    "    random_state=1), X_train, y_train)\n",
    "# SVM\n",
    "SVM_Normal = classify(model_2, svm.SVC(), X_train, y_train)\n",
    "\n",
    "models = []\n",
    "models.append(('LR Normal N=2', LR_Normal))\n",
    "models.append(('SVM Normal N=2', SVM_Normal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Normal N=2\n",
      "0.7464788732394366\n",
      "SVM Normal N=2\n",
      "0.7645875251509054\n"
     ]
    }
   ],
   "source": [
    "file = open('results/output_better.txt', 'w+')\n",
    "file.close()\n",
    "\n",
    "outfile = open(\"results/output_better.txt\", \"a\")\n",
    "for i, v in models:\n",
    "    print(i)\n",
    "    accuracy = metrics.accuracy_score(y_test, v.predict(X_test))\n",
    "    confusion_matrix = metrics.confusion_matrix(y_test, v.predict(X_test))\n",
    "    print('========= {} Model Test Results ==========='.format(i), file=outfile)\n",
    "    print(' ', file=outfile)\n",
    "    print(\"Model Accuracy:\" \"\\n\", accuracy, file=outfile)\n",
    "    print(accuracy)\n",
    "    print(' ', file=outfile)\n",
    "    print(\"Confusion matrix:\" \"\\n\", confusion_matrix, file=outfile)\n",
    "    print(' ', file=outfile)\n",
    "outfile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_seed, X_expand, y_seed, y_expand = train_test_split(X_train,\n",
    "                                                      y_train, test_size=0.99,\n",
    "                                                      random_state=41,\n",
    "                                                      stratify=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_seed:  19\n",
      "X_expand:  1966\n"
     ]
    }
   ],
   "source": [
    "print('X_seed: ',len(X_seed))\n",
    "print('X_expand: ',len(X_expand))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    11\n",
       "1     8\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_seed.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {'seed_tokenized': X_seed,\n",
    "           'label': y_seed}\n",
    "df_seed = pd.DataFrame(df_dict)\n",
    "df_seed.to_csv('datasets/seed_data.csv', index=False)\n",
    "\n",
    "df_dict = {'expansion_tokenized': X_expand,\n",
    "           'label': y_expand}\n",
    "df_expand = pd.DataFrame(df_dict)\n",
    "df_seed.to_csv('datasets/expand_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity And Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(model, line, k):\n",
    "    \"\"\" Returns a vector containing nearest neighbor scores w.r.t. all\n",
    "        words in the model \"\"\"\n",
    "    # words contains all the words in the corpus\n",
    "    lst1 = model.get_nearest_neighbors(line, k)\n",
    "    v1 = []\n",
    "    l1 = [x[1] for x in lst1]\n",
    "    l10 = [x[0] for x in lst1]\n",
    "    for i in range(len(model.words)):\n",
    "        try:\n",
    "            v1.append(l10[l1.index(model.words[i])])\n",
    "        except:\n",
    "            v1.append(0)\n",
    "    return v1\n",
    "\n",
    "\n",
    "def NN(model: fasttext.FastText._FastText, line: str, K):\n",
    "    \"\"\" Returns k fasttext nearest neighbors of a given string \"\"\"\n",
    "    return model.get_nearest_neighbors(line, k=K)\n",
    "\n",
    "\n",
    "def get_NN(model: fasttext.FastText._FastText, lines: list, k: int):\n",
    "    \"\"\" Returns k nearest neighbor scores of multiple strings\"\"\"\n",
    "    scores = []\n",
    "    for line in lines:\n",
    "        scores.append(score(model, line, k))\n",
    "    return scores\n",
    "\n",
    "def cos_sim(a: np.array, b: np.array):\n",
    "    \"\"\" Returns cosine similarity of two 1d arrays \"\"\"\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    if(norm_a * norm_b == 0.0):\n",
    "        return dot_product / (norm_a * norm_b + 0.001)\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "\n",
    "def sim(x: np.array, y: np.array, sim_type: str):\n",
    "    if(sim_type == 'cosine_sim'):\n",
    "        return cos_sim(x, y)\n",
    "\n",
    "\n",
    "def sim_matrix(A: np.array, B: np.array, sim_type: str):\n",
    "    \"\"\" find similarity score matrix between A and B. \n",
    "        A,B: 2d matrix of embeddings/nearest neighbor scores.\n",
    "        sim_type: String denoting type of similarity.\n",
    "    \"\"\"\n",
    "    m, p = A.shape\n",
    "    p, n = B.shape\n",
    "    C = np.zeros((m, n))\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            C[i][j] = sim(A[i, :], B[:, j], sim_type)\n",
    "    return C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expansion Code (Random Sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Expand_R(model: fasttext.FastText._FastText, \n",
    "             seed_set_tokenised: list, \n",
    "             seed_set_label: list, \n",
    "             expansion_tokenised: list, \n",
    "             expansion_set_labels: list, \n",
    "             batch_size: int, \n",
    "             k_neighbors: int, \n",
    "             random_rate: float):\n",
    "\n",
    "    \"\"\"\n",
    "    Takes seed set and and expands the set using expansion_tokenised.\n",
    "    Batch size: no. of texts to be inserted in one go\n",
    "    k_neighbors: no. of neighbors for getting cosine similarity\n",
    "    random_rate: fraction of amount taken for random sampling\n",
    "    \"\"\"\n",
    "             \n",
    "    seed_TK = copy.deepcopy(seed_set_tokenised)\n",
    "    seed_labels = copy.deepcopy(seed_set_label)\n",
    "    count = len(expansion_set_labels)\n",
    "    M = np.arange(0, count, batch_size)\n",
    "    cnt = int(random_rate * batch_size)\n",
    "    \n",
    "    expansion_predicted_labels = []\n",
    "    expansion_true_labels = []\n",
    "    expansion_accuracy = []\n",
    "\n",
    "    for i in range(1, len(M)):\n",
    "\n",
    "        print(M[i], end=' ')\n",
    "\n",
    "        # select batchwise expansion text\n",
    "        exp_TK = expansion_tokenised[M[i-1]:M[i]]\n",
    "        exp_labels = expansion_set_labels[M[i-1]:M[i]]\n",
    "\n",
    "        # nearest neighbors\n",
    "        seed_NN = get_NN(model, seed_TK, k_neighbors)\n",
    "        exp_NN = get_NN(model, exp_TK, k_neighbors)\n",
    "\n",
    "        A = np.array(seed_NN)\n",
    "        B = np.array(exp_NN).T\n",
    "        C = sim_matrix(A, B, \"cosine_sim\")\n",
    "\n",
    "        # find rowwise (seed) index of highest similarity\n",
    "        Y_ind = np.argmax(C, axis=0)\n",
    "        # get labels\n",
    "        Y = [seed_labels[x] for x in Y_ind]\n",
    "\n",
    "        if(random_rate == 0.0):\n",
    "            # no random sampling\n",
    "            pass\n",
    "        else:\n",
    "            # random sampling\n",
    "            Y_r = random.sample(range(0,len(Y)), cnt)\n",
    "            for j in Y_r:\n",
    "                Y[j] = exp_labels[j]\n",
    "\n",
    "        # calc. expansion accuracy\n",
    "        expansion_predicted_labels.extend(Y)\n",
    "        expansion_true_labels.extend(exp_labels)\n",
    "        expansion_accuracy.append(metrics.accuracy_score(exp_labels, Y))\n",
    "\n",
    "        # expand seed set\n",
    "        seed_labels.extend(Y)\n",
    "        seed_TK.extend(exp_TK)\n",
    "\n",
    "    return seed_TK, seed_labels, expansion_true_labels, expansion_predicted_labels, expansion_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncertainty Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Expand_U(model: fasttext.FastText._FastText, \n",
    "             algorithm: object, \n",
    "             seed_set_tokenised: list, \n",
    "             seed_set_label: list, \n",
    "             expansion_tokenised: list,\n",
    "             expansion_set_labels: list, \n",
    "             batch_size: int,\n",
    "             countMax: int):\n",
    "    \"\"\" Uncertainty sampling.\n",
    "    Expand seed set using expansion_set based on lowest confidance scores.\n",
    "    max_threshold: max. probability for uncertainty selection \"\"\"\n",
    "\n",
    "    seed_TK = copy.deepcopy(seed_set_tokenised)\n",
    "    seed_labels = copy.deepcopy(seed_set_label)\n",
    "    count = len(expansion_set_labels)\n",
    "    M = np.arange(0, count, batch_size)\n",
    "    confusion_matrices = []\n",
    "\n",
    "    # exp_TK_certain will be the list of comments having high proba score\n",
    "    exp_TK_certain = []\n",
    "    exp_TK_certain_labels = []\n",
    "\n",
    "\n",
    "    for i in range(1, len(M)):\n",
    "\n",
    "        #print(M[i], end=' ')\n",
    "\n",
    "        exp_TK = expansion_tokenised[M[i-1]:M[i]]\n",
    "        exp_labels = expansion_set_labels[M[i-1]:M[i]]\n",
    "\n",
    "        # take A as training and B as test and store probs in C\n",
    "        small_model = classify(model, algorithm, seed_TK, seed_labels)\n",
    "        # store classwise prob. scores\n",
    "        C = small_model.predict_proba(exp_TK)\n",
    "        # Uncertainty sampling scores\n",
    "        C_abs_diff = [(abs(x[0] - x[1])) for x in C]\n",
    "\n",
    "        # store accuracies\n",
    "        confusion_matrices.append(metrics.confusion_matrix(exp_labels, small_model.predict(exp_TK)))\n",
    "\n",
    "        # Sort lists in ascending order of probabilities from C_abs_diff\n",
    "        sorted_lists = sorted(zip(exp_labels, exp_TK, C, C_abs_diff), key=lambda x: x[3])\n",
    "        exp_labels, exp_TK, C_sorted, score = [[x[i] for x in sorted_lists] for i in range(4)]\n",
    "\n",
    "        Y_uncertain = []\n",
    "        exp_TK_uncertain = []\n",
    "        for j in range(len(C_sorted)):\n",
    "            max_value = max(C_sorted[j])\n",
    "            max_index = str(np.argmax(C_sorted[j]))\n",
    "\n",
    "            # label the comments whose score is less than threshold\n",
    "            if(j < countMax):\n",
    "\n",
    "                exp_TK_uncertain.append(exp_TK[j])\n",
    "                Y_uncertain.append(exp_labels[j])\n",
    "            else:\n",
    "                exp_TK_certain.append(exp_TK[j])\n",
    "                exp_TK_certain_labels.append(exp_labels[j])\n",
    "\n",
    "        # expand the seed set\n",
    "        seed_labels.extend(Y_uncertain)\n",
    "        seed_TK.extend(exp_TK_uncertain)\n",
    "\n",
    "\n",
    "    return seed_TK, seed_labels, exp_TK_certain, exp_TK_certain_labels, confusion_matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_uncertain_exp_u, y_uncertain_exp_u, X_certain_exp_u, y_certain_exp_u, confusion_matrices = Expand_U(model_2,\n",
    "                                                                          LogisticRegression(random_state=1),\n",
    "                                                                          list(X_seed),\n",
    "                                                                          list(y_seed),\n",
    "                                                                          list(X_expand),\n",
    "                                                                          list(y_expand),\n",
    "                                                                          20,\n",
    "                                                                          1\n",
    "                                                                          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "117\n",
      "1862\n"
     ]
    }
   ],
   "source": [
    "print(len(X_seed))\n",
    "print(len(X_uncertain_exp_u))\n",
    "print(len(X_certain_exp_u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "LR_Normal_u = classify(model_2, LogisticRegression(\n",
    "    random_state=1), X_uncertain_exp_u, y_uncertain_exp_u)\n",
    "# SVM\n",
    "SVM_Normal_u = classify(model_2, svm.SVC(), X_uncertain_exp_u, y_uncertain_exp_u)\n",
    "\n",
    "# GNB\n",
    "GNB_Normal_u = classify(model_2, GaussianNB(), X_uncertain_exp_u, y_uncertain_exp_u)\n",
    "\n",
    "# DT\n",
    "DT_Normal_u = classify(model_2, DecisionTreeClassifier(),X_uncertain_exp_u, y_uncertain_exp_u)\n",
    "\n",
    "models_u = []\n",
    "models_u.append(('LR Normal N=2', LR_Normal_u))\n",
    "models_u.append(('SVM Normal N=2', SVM_Normal_u))\n",
    "models_u.append(('GNB Normal N=2', GNB_Normal_u))\n",
    "models_u.append(('DT Normal N=2', DT_Normal_u))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Normal N=2\n",
      "0.7283702213279678\n",
      "SVM Normal N=2\n",
      "0.7082494969818913\n",
      "GNB Normal N=2\n",
      "0.7142857142857143\n",
      "DT Normal N=2\n",
      "0.6016096579476862\n"
     ]
    }
   ],
   "source": [
    "file = open('results/output_uncertain.txt', 'w+')\n",
    "file.close()\n",
    "\n",
    "outfile = open(\"results/output_uncertain.txt\", \"a\")\n",
    "for i, v in models_u:\n",
    "    print(i)\n",
    "    accuracy = metrics.accuracy_score(y_test, v.predict(X_test))\n",
    "    confusion_matrix = metrics.confusion_matrix(y_test, v.predict(X_test))\n",
    "    print('========= {} Model Test Results ==========='.format(i), file=outfile)\n",
    "    print(' ', file=outfile)\n",
    "    print(\"Model Accuracy:\" \"\\n\", accuracy, file=outfile)\n",
    "    print(accuracy)\n",
    "    print(' ', file=outfile)\n",
    "    print(\"Confusion matrix:\" \"\\n\", confusion_matrix, file=outfile)\n",
    "    print(' ', file=outfile)\n",
    "outfile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 0, 3, 6, 9, 12, 15, 18, 21, 24, 27, 0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 0, 6, 12, 18, 24, 30, 36, 42, 48, 54, 0, 7, 14, 21, 28, 35, 42, 49, 56, 63, 0, 8, 16, 24, 32, 40, 48, 56, 64, 72, 0, 9, 18, 27, 36, 45, 54, 63, 72, 81]\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def process(i,j):\n",
    "    return i * j\n",
    "\n",
    "\n",
    "results = Parallel(n_jobs=2)(delayed(process)(i,j) for i in range(10) for j in range(10))\n",
    "print(results)  # prints [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\AJAY BISWAS\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 436, in _process_worker\n    r = call_item()\n  File \"C:\\Users\\AJAY BISWAS\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 288, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"C:\\Users\\AJAY BISWAS\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\AJAY BISWAS\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\AJAY BISWAS\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\AJAYBI~1\\AppData\\Local\\Temp/ipykernel_7740/694413483.py\", line 26, in process\nTypeError: 'int' object is not iterable\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\AJAYBI~1\\AppData\\Local\\Temp/ipykernel_7740/694413483.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodels\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0malgorithms\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbest_amounts\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutp_algorithms\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1056\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1057\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1058\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    933\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 935\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    936\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    937\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Python39\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    443\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    446\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Python39\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    388\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 390\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    391\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m                 \u001b[1;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "models = [('fasttext', 'models/ft_unsupervised_N_2_gen.bin')]\n",
    "algorithms = ['lr', 'svm']\n",
    "batch_sizes = [10, 20, 30, 40, 50, 100]\n",
    "best_amounts = [1, 2, 5]\n",
    "outp_algorithms = ['lr', 'svm']\n",
    "\n",
    "def process(models, algorithms,batch_sizes,best_amounts,outp_algorithms):\n",
    "\n",
    "    big_confusion = []\n",
    "    big_results = []\n",
    "    \n",
    "    for md in models:\n",
    "        mdl = None\n",
    "        if(md[0] == 'fasttext'):\n",
    "            mdl = fasttext.load_model(md[1])\n",
    "\n",
    "        for algorithm in algorithms:\n",
    "            alg = None\n",
    "            if(algorithm == 'lr'):\n",
    "                alg = LogisticRegression()\n",
    "            elif(algorithm == 'svm'):\n",
    "                alg = svm.SVC()\n",
    "\n",
    "            for batch_size in batch_sizes:\n",
    "                for best_amount in best_amounts:\n",
    "                    for outp_algorithm in outp_algorithms:\n",
    "\n",
    "                        X_uncertain_exp_u,\n",
    "                        y_uncertain_exp_u,\n",
    "                        X_certain_exp_u,\n",
    "                        y_certain_exp_u,\n",
    "                        confusion_matrices = Expand_U(mdl,\n",
    "                                                    alg,\n",
    "                                                    list(X_seed),\n",
    "                                                    list(y_seed),\n",
    "                                                    list(X_expand),\n",
    "                                                    list(y_expand),\n",
    "                                                    batch_size,\n",
    "                                                    best_amount\n",
    "                                                    )\n",
    "\n",
    "                        alg2 = None\n",
    "                        if(outp_algorithm == 'lr'):\n",
    "                            alg2 = LogisticRegression()\n",
    "                        elif(outp_algorithm == 'svm'):\n",
    "                            alg2 = svm.SVC()\n",
    "\n",
    "                        # classify\n",
    "                        c_model = classify(mdl, alg2,X_uncertain_exp_u, y_uncertain_exp_u)\n",
    "                        c_accuracy = metrics.accuracy_score(y_test, c_model.predict(X_test))\n",
    "\n",
    "                        big_results.append((algorithm,batch_size,best_amount,outp_algorithm,c_accuracy))\n",
    "                        #print(algorithm, ',', batch_size, ',', best_amount,',', outp_algorithm, ',', c_accuracy)\n",
    "                        big_confusion.append(confusion_matrices)\n",
    "    return big_results, big_confusion\n",
    "\n",
    "\n",
    "\n",
    "results = Parallel(n_jobs=4)(delayed(process)(m,i,j,k,l) for m in models for i in algorithms for j in batch_sizes for k in best_amounts for l in outp_algorithms )\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algorithm , batch_size ,  best_amount ,  outp_algorithm , c_accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr , 10 , 1 , lr , 0.7283702213279678\n",
      "lr , 10 , 1 , svm , 0.7082494969818913\n",
      "lr , 10 , 2 , lr , 0.7283702213279678\n",
      "lr , 10 , 2 , svm , 0.7082494969818913\n",
      "lr , 10 , 5 , lr , 0.7283702213279678\n",
      "lr , 10 , 5 , svm , 0.7082494969818913\n",
      "lr , 20 , 1 , lr , 0.7283702213279678\n",
      "lr , 20 , 1 , svm , 0.7082494969818913\n",
      "lr , 20 , 2 , lr , 0.7283702213279678\n",
      "lr , 20 , 2 , svm , 0.7082494969818913\n",
      "lr , 20 , 5 , lr , 0.7283702213279678\n",
      "lr , 20 , 5 , svm , 0.7082494969818913\n",
      "lr , 30 , 1 , lr , 0.7283702213279678\n",
      "lr , 30 , 1 , svm , 0.7082494969818913\n",
      "lr , 30 , 2 , lr , 0.7283702213279678\n",
      "lr , 30 , 2 , svm , 0.7082494969818913\n",
      "lr , 30 , 5 , lr , 0.7283702213279678\n",
      "lr , 30 , 5 , svm , 0.7082494969818913\n",
      "lr , 40 , 1 , lr , 0.7283702213279678\n",
      "lr , 40 , 1 , svm , 0.7082494969818913\n",
      "lr , 40 , 2 , lr , 0.7283702213279678\n",
      "lr , 40 , 2 , svm , 0.7082494969818913\n",
      "lr , 40 , 5 , lr , 0.7283702213279678\n",
      "lr , 40 , 5 , svm , 0.7082494969818913\n",
      "lr , 50 , 1 , lr , 0.7283702213279678\n",
      "lr , 50 , 1 , svm , 0.7082494969818913\n",
      "lr , 50 , 2 , lr , 0.7283702213279678\n",
      "lr , 50 , 2 , svm , 0.7082494969818913\n",
      "lr , 50 , 5 , lr , 0.7283702213279678\n",
      "lr , 50 , 5 , svm , 0.7082494969818913\n",
      "lr , 100 , 1 , lr , 0.7283702213279678\n",
      "lr , 100 , 1 , svm , 0.7082494969818913\n",
      "lr , 100 , 2 , lr , 0.7283702213279678\n",
      "lr , 100 , 2 , svm , 0.7082494969818913\n",
      "lr , 100 , 5 , lr , 0.7283702213279678\n",
      "lr , 100 , 5 , svm , 0.7082494969818913\n",
      "svm , 10 , 1 , lr , 0.7283702213279678\n",
      "svm , 10 , 1 , svm , 0.7082494969818913\n",
      "svm , 10 , 2 , lr , 0.7283702213279678\n",
      "svm , 10 , 2 , svm , 0.7082494969818913\n",
      "svm , 10 , 5 , lr , 0.7283702213279678\n",
      "svm , 10 , 5 , svm , 0.7082494969818913\n",
      "svm , 20 , 1 , lr , 0.7283702213279678\n",
      "svm , 20 , 1 , svm , 0.7082494969818913\n",
      "svm , 20 , 2 , lr , 0.7283702213279678\n",
      "svm , 20 , 2 , svm , 0.7082494969818913\n",
      "svm , 20 , 5 , lr , 0.7283702213279678\n",
      "svm , 20 , 5 , svm , 0.7082494969818913\n",
      "svm , 30 , 1 , lr , 0.7283702213279678\n",
      "svm , 30 , 1 , svm , 0.7082494969818913\n",
      "svm , 30 , 2 , lr , 0.7283702213279678\n",
      "svm , 30 , 2 , svm , 0.7082494969818913\n",
      "svm , 30 , 5 , lr , 0.7283702213279678\n",
      "svm , 30 , 5 , svm , 0.7082494969818913\n",
      "svm , 40 , 1 , lr , 0.7283702213279678\n",
      "svm , 40 , 1 , svm , 0.7082494969818913\n",
      "svm , 40 , 2 , lr , 0.7283702213279678\n",
      "svm , 40 , 2 , svm , 0.7082494969818913\n",
      "svm , 40 , 5 , lr , 0.7283702213279678\n",
      "svm , 40 , 5 , svm , 0.7082494969818913\n",
      "svm , 50 , 1 , lr , 0.7283702213279678\n",
      "svm , 50 , 1 , svm , 0.7082494969818913\n",
      "svm , 50 , 2 , lr , 0.7283702213279678\n",
      "svm , 50 , 2 , svm , 0.7082494969818913\n",
      "svm , 50 , 5 , lr , 0.7283702213279678\n",
      "svm , 50 , 5 , svm , 0.7082494969818913\n",
      "svm , 100 , 1 , lr , 0.7283702213279678\n",
      "svm , 100 , 1 , svm , 0.7082494969818913\n",
      "svm , 100 , 2 , lr , 0.7283702213279678\n",
      "svm , 100 , 2 , svm , 0.7082494969818913\n",
      "svm , 100 , 5 , lr , 0.7283702213279678\n",
      "svm , 100 , 5 , svm , 0.7082494969818913\n"
     ]
    }
   ],
   "source": [
    "print('algorithm',',','batch_size',', ','best_amount',', ','outp_algorithm',',','c_accuracy')\n",
    "\n",
    "models = [('fasttext', 'models/ft_unsupervised_N_2.bin')]\n",
    "algorithms = ['lr', 'svm']\n",
    "batch_sizes = [10, 20, 30, 40, 50, 100]\n",
    "best_amounts = [1, 2, 5]\n",
    "outp_algorithms = ['lr', 'svm']\n",
    "\n",
    "big_confusion = []\n",
    "big_results = []\n",
    "\n",
    "for md in models:\n",
    "    mdl = None\n",
    "    if(md[0] == 'fasttext'):\n",
    "        mdl = fasttext.load_model(md[1])\n",
    "\n",
    "    for algorithm in algorithms:\n",
    "        alg = None\n",
    "        if(algorithm == 'lr'):\n",
    "            alg = LogisticRegression()\n",
    "        elif(algorithm == 'svm'):\n",
    "            alg = svm.SVC(probability=True)\n",
    "\n",
    "        for batch_size in batch_sizes:\n",
    "            for best_amount in best_amounts:\n",
    "                 for outp_algorithm in outp_algorithms:\n",
    "\n",
    "                    X_uncertain_exp_u,\n",
    "                    y_uncertain_exp_u,\n",
    "                    X_certain_exp_u,\n",
    "                    y_certain_exp_u,\n",
    "                    confusion_matrices = Expand_U(mdl,\n",
    "                                                    alg,\n",
    "                                                    list(X_seed),\n",
    "                                                    list(y_seed),\n",
    "                                                    list(X_expand),\n",
    "                                                    list(y_expand),\n",
    "                                                    batch_size,\n",
    "                                                    best_amount\n",
    "                                                    )\n",
    "\n",
    "                    alg2 = None\n",
    "                    if(outp_algorithm == 'lr'):\n",
    "                        alg2 = LogisticRegression()\n",
    "                    elif(outp_algorithm == 'svm'):\n",
    "                        alg2 = svm.SVC()\n",
    "\n",
    "                    # classify\n",
    "                    c_model = classify(\n",
    "                        mdl, alg2, X_uncertain_exp_u, y_uncertain_exp_u)\n",
    "                    c_accuracy = metrics.accuracy_score(\n",
    "                        y_test, c_model.predict(X_test))\n",
    "\n",
    "                    big_results.append((algorithm, batch_size, best_amount, outp_algorithm, c_accuracy))\n",
    "                    print(algorithm, ',', batch_size, ',', best_amount,',', outp_algorithm, ',', c_accuracy)\n",
    "                    big_confusion.append(confusion_matrices)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee5cc6fef2d70a7e71ee3826687cbd150f18158e0b1eef11d4f4f92bb920e304"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
