{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\AJAY\n",
      "[nltk_data]     BISWAS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import fasttext\n",
    "import pandas as pd\n",
    "import random\n",
    "import itertools\n",
    "from resources.tokTT import CommentTokenizer as CT\n",
    "from resources.basicIO import InputOutput as IO\n",
    "from resources.basicIO import InputOutput as IO\n",
    "from resources.filterLang import FilterLanguage as FL\n",
    "from resources.fasttext_transformer import classify\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import fasttext\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "from scipy.spatial import distance\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "#from sklearn.linear_model import LassoLars\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import copy\n",
    "import scipy\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import validation_curve\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Fasttext Unsupervised Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "model_2 = fasttext.load_model(\"models/ft_unsupervised_N_2.bin\")\n",
    "model_3 = fasttext.load_model(\"models/ft_unsupervised_N_3.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random sample\n",
    "text = IO.load_csv_col('datasets/random_sample.csv', 'comment')\n",
    "text_labels = IO.load_csv_col('datasets/random_sample.csv', 'label')\n",
    "text_labels = list(map(str, map(int, text_labels)))\n",
    "text_TK = [CT.tokenize(x) for x in text]\n",
    "\n",
    "df_dict = {'raw_comment': text,\n",
    "           'tokenized_comment': text_TK, 'label': text_labels}\n",
    "df_sample = pd.DataFrame(df_dict)\n",
    "df_sample.to_csv('datasets/random_sample_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Unnecessary Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unnecessary comments\n",
    "df_sample = df_sample.drop([x for x in range(len(df_sample)) if int(\n",
    "    df_sample['label'][x]) != 0 and int(df_sample['label'][x]) != 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_comment</th>\n",
       "      <th>tokenized_comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4477</th>\n",
       "      <td>all the flags are sikh religious flags. they d...</td>\n",
       "      <td>all the flag be sikh religious flag they do no...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4478</th>\n",
       "      <td>Salute the farmers</td>\n",
       "      <td>salute the farmer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4479</th>\n",
       "      <td>Let's also remember how many farmers lost thei...</td>\n",
       "      <td>let be also remember how many farmer lose thei...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4480</th>\n",
       "      <td>ABP news you guys dont have heart 8 farmers go...</td>\n",
       "      <td>abp news you guy dont have heart 8 farmer get ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4481</th>\n",
       "      <td>These new laws actually give the power to the ...</td>\n",
       "      <td>these new law actually give the power to the f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            raw_comment  \\\n",
       "4477  all the flags are sikh religious flags. they d...   \n",
       "4478                                 Salute the farmers   \n",
       "4479  Let's also remember how many farmers lost thei...   \n",
       "4480  ABP news you guys dont have heart 8 farmers go...   \n",
       "4481  These new laws actually give the power to the ...   \n",
       "\n",
       "                                      tokenized_comment label  \n",
       "4477  all the flag be sikh religious flag they do no...     1  \n",
       "4478                                  salute the farmer     0  \n",
       "4479  let be also remember how many farmer lose thei...     0  \n",
       "4480  abp news you guy dont have heart 8 farmer get ...     0  \n",
       "4481  these new law actually give the power to the f...     1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1847\n",
       "1    1407\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample['label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_sample['tokenized_comment'],\n",
    "                                                    df_sample['label'], test_size=0.2,\n",
    "                                                    random_state=37,\n",
    "                                                    stratify=df_sample['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  2603\n",
      "X_test:  651\n"
     ]
    }
   ],
   "source": [
    "print('X_train: ' ,len(X_train))\n",
    "print('X_test: ' ,len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"SVM\": make_pipeline(StandardScaler(), svm.SVC(probability=True, kernel='rbf')),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=19),\n",
    "    \"LR\": make_pipeline(StandardScaler(), LogisticRegression(solver='lbfgs', penalty='l2', C=0.05, max_iter=10000, random_state=2)),\n",
    "    \"MLP\": make_pipeline(StandardScaler(), MLPClassifier(max_iter=1000, activation='logistic'))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "LR_Normal = classify(model_2, classifiers[\"LR\"], X_train, y_train)\n",
    "# SVM\n",
    "SVM_Normal = classify(model_2, classifiers[\"SVM\"], X_train, y_train)\n",
    "# KNN\n",
    "KNN_Normal = classify(model_2, classifiers[\"KNN\"], X_train, y_train)\n",
    "# MLP\n",
    "MLP_Normal = classify(model_2, classifiers[\"MLP\"], X_train, y_train)\n",
    "\n",
    "\n",
    "# Logistic Regression\n",
    "LR_Normal3 = classify(model_3, classifiers[\"LR\"], X_train, y_train)\n",
    "# SVM\n",
    "SVM_Normal3 = classify(model_3, classifiers[\"SVM\"], X_train, y_train)\n",
    "# KNN\n",
    "KNN_Normal3 = classify(model_3, classifiers[\"KNN\"], X_train, y_train)\n",
    "# MLP\n",
    "MLP_Normal3 = classify(model_3, classifiers[\"MLP\"], X_train, y_train)\n",
    "\n",
    "models = []\n",
    "models.append(('LR Normal N=2', LR_Normal))\n",
    "models.append(('SVM Normal N=2', SVM_Normal))\n",
    "models.append(('KNN Normal N=2', KNN_Normal))\n",
    "models.append(('MLP Normal N=2', MLP_Normal))\n",
    "models.append(('LR Normal N=3', LR_Normal3))\n",
    "models.append(('SVM Normal N=3', SVM_Normal3))\n",
    "models.append(('KNN Normal N=3', KNN_Normal3))\n",
    "models.append(('MLP Normal N=3', MLP_Normal3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Normal N=2\n",
      "0.7511520737327189\n",
      "SVM Normal N=2\n",
      "0.7895545314900153\n",
      "KNN Normal N=2\n",
      "0.7112135176651305\n",
      "MLP Normal N=2\n",
      "0.7680491551459293\n",
      "LR Normal N=3\n",
      "0.7588325652841782\n",
      "SVM Normal N=3\n",
      "0.7880184331797235\n",
      "KNN Normal N=3\n",
      "0.7112135176651305\n",
      "MLP Normal N=3\n",
      "0.7788018433179723\n"
     ]
    }
   ],
   "source": [
    "file = open('results/output_normal.txt', 'w+')\n",
    "file.close()\n",
    "\n",
    "outfile = open(\"results/output_normal.txt\", \"a\")\n",
    "for i, v in models:\n",
    "    print(i)\n",
    "    accuracy = metrics.accuracy_score(y_test, v.predict(X_test))\n",
    "    confusion_matrix = metrics.confusion_matrix(y_test, v.predict(X_test))\n",
    "    print('========= {} Model Test Results ==========='.format(i), file=outfile)\n",
    "    print(' ', file=outfile)\n",
    "    print(\"Model Accuracy:\" \"\\n\", accuracy, file=outfile)\n",
    "    print(accuracy)\n",
    "    print(' ', file=outfile)\n",
    "    print(\"Confusion matrix:\" \"\\n\", confusion_matrix, file=outfile)\n",
    "    print(' ', file=outfile)\n",
    "outfile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_seed, X_expand, y_seed, y_expand = train_test_split(X_train,\n",
    "                                                      y_train, test_size=0.99,\n",
    "                                                      random_state=40,\n",
    "                                                      stratify=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_seed:  26\n",
      "X_expand:  2577\n"
     ]
    }
   ],
   "source": [
    "print('X_seed: ',len(X_seed))\n",
    "print('X_expand: ',len(X_expand))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    15\n",
       "1    11\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_seed.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {'seed_tokenized': X_seed,\n",
    "           'label': y_seed}\n",
    "df_seed = pd.DataFrame(df_dict)\n",
    "df_seed.to_csv('datasets/seed_data.csv', index=False)\n",
    "\n",
    "df_dict = {'expansion_tokenized': X_expand,\n",
    "           'label': y_expand}\n",
    "df_expand = pd.DataFrame(df_dict)\n",
    "df_seed.to_csv('datasets/expand_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity And Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(model, line, k):\n",
    "    \"\"\" Returns a vector containing nearest neighbor scores w.r.t. all\n",
    "        words in the model \"\"\"\n",
    "    # words contains all the words in the corpus\n",
    "    lst1 = model.get_nearest_neighbors(line, k)\n",
    "    v1 = []\n",
    "    l1 = [x[1] for x in lst1]\n",
    "    l10 = [x[0] for x in lst1]\n",
    "    for i in range(len(model.words)):\n",
    "        try:\n",
    "            v1.append(l10[l1.index(model.words[i])])\n",
    "        except:\n",
    "            v1.append(0)\n",
    "    return v1\n",
    "\n",
    "\n",
    "def NN(model: fasttext.FastText._FastText, line: str, K):\n",
    "    \"\"\" Returns k fasttext nearest neighbors of a given string \"\"\"\n",
    "    return model.get_nearest_neighbors(line, k=K)\n",
    "\n",
    "\n",
    "def get_NN(model: fasttext.FastText._FastText, lines: list, k: int):\n",
    "    \"\"\" Returns k nearest neighbor scores of multiple strings\"\"\"\n",
    "    scores = []\n",
    "    for line in lines:\n",
    "        scores.append(score(model, line, k))\n",
    "    return scores\n",
    "\n",
    "def cos_sim(a: np.array, b: np.array):\n",
    "    \"\"\" Returns cosine similarity of two 1d arrays \"\"\"\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    if(norm_a * norm_b == 0.0):\n",
    "        return dot_product / (norm_a * norm_b + 0.001)\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "\n",
    "def sim(x: np.array, y: np.array, sim_type: str):\n",
    "    if(sim_type == 'cosine_sim'):\n",
    "        return cos_sim(x, y)\n",
    "\n",
    "\n",
    "def sim_matrix(A: np.array, B: np.array, sim_type: str):\n",
    "    \"\"\" find similarity score matrix between A and B. \n",
    "        A,B: 2d matrix of embeddings/nearest neighbor scores.\n",
    "        sim_type: String denoting type of similarity.\n",
    "    \"\"\"\n",
    "    m, p = A.shape\n",
    "    p, n = B.shape\n",
    "    C = np.zeros((m, n))\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            C[i][j] = sim(A[i, :], B[:, j], sim_type)\n",
    "    return C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expansion Code (Random Sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Expand_R(model: fasttext.FastText._FastText, \n",
    "             seed_set_tokenised: list, \n",
    "             seed_set_label: list, \n",
    "             expansion_tokenised: list, \n",
    "             expansion_set_labels: list, \n",
    "             batch_size: int, \n",
    "             k_neighbors: int, \n",
    "             random_rate: float):\n",
    "\n",
    "    \"\"\"\n",
    "    Takes seed set and and expands the set using expansion_tokenised.\n",
    "    Batch size: no. of texts to be inserted in one go\n",
    "    k_neighbors: no. of neighbors for getting cosine similarity\n",
    "    random_rate: fraction of amount taken for random sampling\n",
    "    \"\"\"\n",
    "             \n",
    "    seed_TK = copy.deepcopy(seed_set_tokenised)\n",
    "    seed_labels = copy.deepcopy(seed_set_label)\n",
    "    count = len(expansion_set_labels)\n",
    "    M = np.arange(0, count, batch_size)\n",
    "    cnt = int(random_rate * batch_size)\n",
    "    \n",
    "    expansion_predicted_labels = []\n",
    "    expansion_true_labels = []\n",
    "\n",
    "    for i in range(1, len(M)):\n",
    "\n",
    "        #print(M[i], end=' ')\n",
    "\n",
    "        # select batchwise expansion text\n",
    "        exp_TK = expansion_tokenised[M[i-1]:M[i]]\n",
    "        exp_labels = expansion_set_labels[M[i-1]:M[i]]\n",
    "\n",
    "        # nearest neighbors\n",
    "        seed_NN = get_NN(model, seed_TK, k_neighbors)\n",
    "        exp_NN = get_NN(model, exp_TK, k_neighbors)\n",
    "\n",
    "        A = np.array(seed_NN)\n",
    "        B = np.array(exp_NN).T\n",
    "        C = sim_matrix(A, B, \"cosine_sim\")\n",
    "\n",
    "        # find rowwise (seed) index of highest similarity\n",
    "        Y_ind = np.argmax(C, axis=0)\n",
    "        # get labels\n",
    "        Y = [seed_labels[x] for x in Y_ind]\n",
    "\n",
    "        if(random_rate == 0.0):\n",
    "            # no random sampling\n",
    "            pass\n",
    "        else:\n",
    "            # random sampling\n",
    "            Y_r = random.sample(range(0,len(Y)), cnt)\n",
    "            for j in Y_r:\n",
    "                Y[j] = exp_labels[j]\n",
    "\n",
    "        # calc. expansion accuracy\n",
    "        expansion_predicted_labels.extend(Y)\n",
    "        expansion_true_labels.extend(exp_labels)\n",
    "\n",
    "        # expand seed set\n",
    "        seed_labels.extend(Y)\n",
    "        seed_TK.extend(exp_TK)\n",
    "\n",
    "    return seed_TK, seed_labels, expansion_true_labels, expansion_predicted_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model  batch_size,  Random Rate, expanded_size,  outp_algorithm,  c_accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/ft_unsupervised_N_2.bin , 20 , 0.2 , 2586 , LR , 0.7096774193548387\n",
      "models/ft_unsupervised_N_2.bin , 20 , 0.2 , 2586 , SVM , 0.706605222734255\n",
      "models/ft_unsupervised_N_2.bin , 20 , 0.2 , 2586 , KNN , 0.6712749615975423\n",
      "models/ft_unsupervised_N_2.bin , 20 , 0.2 , 2586 , MLP , 0.6989247311827957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/ft_unsupervised_N_3.bin , 20 , 0.2 , 2586 , LR , 0.6897081413210445\n",
      "models/ft_unsupervised_N_3.bin , 20 , 0.2 , 2586 , SVM , 0.7035330261136713\n",
      "models/ft_unsupervised_N_3.bin , 20 , 0.2 , 2586 , KNN , 0.6666666666666666\n",
      "models/ft_unsupervised_N_3.bin , 20 , 0.2 , 2586 , MLP , 0.6697388632872504\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('model ', 'batch_size, ', 'Random Rate, ' 'expanded_size, ', 'outp_algorithm, ', 'c_accuracy')\n",
    "\n",
    "models = [('fasttext', 'models/ft_unsupervised_N_2.bin'),\n",
    "          ('fasttext', 'models/ft_unsupervised_N_3.bin')]\n",
    "\n",
    "batch_sizes = [20]\n",
    "random_rates = [0.2]\n",
    "outp_algorithms = ['LR','SVM','KNN','MLP']\n",
    "big_results_r = []\n",
    "big_confusion_r = []\n",
    "\n",
    "for md in models:\n",
    "    mdl = None\n",
    "    if(md[0] == 'fasttext'):\n",
    "        mdl = fasttext.load_model(md[1])\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        for random_rate in random_rates:\n",
    "            seed_TK, seed_labels, _tk, _labels = Expand_R(mdl,\n",
    "                                                list(\n",
    "                                                    X_seed),\n",
    "                                                list(\n",
    "                                                    y_seed),\n",
    "                                                list(\n",
    "                                                    X_expand),\n",
    "                                                list(\n",
    "                                                    y_expand),\n",
    "                                                batch_size,\n",
    "                                                40,\n",
    "                                                random_rate)\n",
    "                                                                              \n",
    "            for outp_algorithm in outp_algorithms:    \n",
    "\n",
    "                alg2 = classifiers[outp_algorithm]\n",
    "\n",
    "                # classify\n",
    "                c_model = classify(mdl, alg2, seed_TK, seed_labels)\n",
    "                y_predict = c_model.predict(X_test)\n",
    "                c_accuracy = metrics.accuracy_score(y_test, y_predict)\n",
    "                c_prfsw = list(precision_recall_fscore_support(y_test, y_predict, average='weighted'))\n",
    "                c_prfsmi = list(precision_recall_fscore_support(y_test, y_predict, average='micro'))\n",
    "\n",
    "                l1 = [md[1], batch_size, random_rate, len(seed_TK), outp_algorithm, c_accuracy]\n",
    "                l1.extend(c_prfsw)\n",
    "                l1.extend(c_prfsmi)\n",
    "\n",
    "                big_results_r.append(l1)\n",
    "                big_confusion_r.append(metrics.confusion_matrix(y_test, c_model.predict(X_test)))\n",
    "                print(md[1], ',', batch_size, ',', random_rate, ',', len(seed_TK), ',', outp_algorithm, ',', c_accuracy)\n",
    "    del mdl\n",
    "    gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[283  87]\n",
      "  [102 179]]\n",
      "\n",
      " [[293  77]\n",
      "  [114 167]]\n",
      "\n",
      " [[236 134]\n",
      "  [ 80 201]]\n",
      "\n",
      " [[276  94]\n",
      "  [102 179]]\n",
      "\n",
      " [[277  93]\n",
      "  [109 172]]\n",
      "\n",
      " [[288  82]\n",
      "  [111 170]]\n",
      "\n",
      " [[239 131]\n",
      "  [ 86 195]]\n",
      "\n",
      " [[266 104]\n",
      "  [111 170]]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "res_df = pd.DataFrame(big_results_r)\n",
    "res_df.to_csv(\"results/big_results_r3.csv\", index=False, header=False)\n",
    "\n",
    "print(np.array(big_confusion_r))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncertainty Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Expand_U(model: fasttext.FastText._FastText, \n",
    "             algorithm: object, \n",
    "             seed_set_tokenised: list, \n",
    "             seed_set_label: list, \n",
    "             expansion_tokenised: list,\n",
    "             expansion_set_labels: list, \n",
    "             batch_size: int,\n",
    "             countMax: int):\n",
    "    \"\"\" Uncertainty sampling.\n",
    "    Expand seed set using expansion_set based on lowest confidance scores.\n",
    "    max_threshold: max. probability for uncertainty selection \"\"\"\n",
    "\n",
    "    seed_TK = copy.deepcopy(seed_set_tokenised)\n",
    "    seed_labels = copy.deepcopy(seed_set_label)\n",
    "    count = len(expansion_set_labels)\n",
    "    M = np.arange(0, count, batch_size)\n",
    "\n",
    "    # exp_TK_certain will be the list of comments having high proba score\n",
    "    exp_TK_certain = []\n",
    "    exp_TK_certain_labels = []\n",
    "\n",
    "\n",
    "    for i in range(1, len(M)):\n",
    "\n",
    "        #print(M[i], end=' ')\n",
    "\n",
    "        exp_TK = expansion_tokenised[M[i-1]:M[i]]\n",
    "        exp_labels = expansion_set_labels[M[i-1]:M[i]]\n",
    "\n",
    "        # take A as training and B as test and store probs in C\n",
    "        small_model = classify(model, algorithm, seed_TK, seed_labels)\n",
    "        # store classwise prob. scores\n",
    "        C = small_model.predict_proba(exp_TK)\n",
    "        # Uncertainty sampling scores\n",
    "        C_abs_diff = [(abs(x[0] - x[1])) for x in C]\n",
    "\n",
    "        y_predict = small_model.predict(X_test)\n",
    "        c_accuracy = metrics.accuracy_score(y_test, y_predict)\n",
    "        c_prfsw = list(precision_recall_fscore_support(\n",
    "            y_test, y_predict, average='weighted'))\n",
    "        c_prfsmi = list(precision_recall_fscore_support(\n",
    "            y_test, y_predict, average='micro'))\n",
    "        \n",
    "        l1 = []\n",
    "        l1.append(c_accuracy)\n",
    "        l1.extend(c_prfsw[0:3])\n",
    "        l1.extend(c_prfsmi[0:3])\n",
    "\n",
    "        # Sort lists in ascending order of probabilities from C_abs_diff\n",
    "        sorted_lists = sorted(zip(exp_labels, exp_TK, C, C_abs_diff), key=lambda x: x[3])\n",
    "        exp_labels, exp_TK, C_sorted, score = [[x[i] for x in sorted_lists] for i in range(4)]\n",
    "\n",
    "        Y_uncertain = []\n",
    "        exp_TK_uncertain = []\n",
    "        for j in range(len(C_sorted)):\n",
    "            #max_value = max(C_sorted[j])\n",
    "            #max_index = str(np.argmax(C_sorted[j]))\n",
    "\n",
    "            # label the comments whose score is less than threshold\n",
    "            if(j < countMax):\n",
    "\n",
    "                exp_TK_uncertain.append(exp_TK[j])\n",
    "                Y_uncertain.append(exp_labels[j])\n",
    "            else:\n",
    "                exp_TK_certain.append(exp_TK[j])\n",
    "                exp_TK_certain_labels.append(exp_labels[j])\n",
    "\n",
    "        # expand the seed set\n",
    "        seed_labels.extend(Y_uncertain)\n",
    "        seed_TK.extend(exp_TK_uncertain)\n",
    "\n",
    "\n",
    "    return seed_TK, seed_labels, exp_TK_certain, exp_TK_certain_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "#print('model ', 'algorithm, ', 'batch_size, ', 'best_amount, ', 'expanded_size, ', 'outp_algorithm, ', 'c_accuracy')\n",
    "\n",
    "models = [('fasttext', 'models/ft_unsupervised_N_2.bin'),\n",
    "          ('fasttext', 'models/ft_unsupervised_N_3.bin')\n",
    "          ]\n",
    "algorithms = ['LR', 'SVM', 'KNN', 'MLP']\n",
    "batch_sizes = [20]\n",
    "best_amounts = [5]\n",
    "big_results = []\n",
    "big_confusion = []\n",
    "for md in models:\n",
    "    mdl = None\n",
    "    if(md[0] == 'fasttext'):\n",
    "        mdl = fasttext.load_model(md[1])\n",
    "\n",
    "    for algorithm in algorithms:\n",
    "        alg = classifiers[algorithm]\n",
    "\n",
    "        for batch_size in batch_sizes:\n",
    "\n",
    "            for best_amount in best_amounts:\n",
    "\n",
    "                X_uncertain_exp_u, y_uncertain_exp_u, X_certain_exp_u, y_certain_exp_u = Expand_U(mdl,\n",
    "                                                                                        alg,\n",
    "                                                                                        list(\n",
    "                                                                                            X_seed),\n",
    "                                                                                        list(\n",
    "                                                                                            y_seed),\n",
    "                                                                                        list(\n",
    "                                                                                            X_expand),\n",
    "                                                                                        list(\n",
    "                                                                                            y_expand),\n",
    "                                                                                        batch_size,\n",
    "                                                                                        best_amount\n",
    "                                                                                        )\n",
    "\n",
    "                # classify\n",
    "                c_model = classify(mdl, alg, X_uncertain_exp_u, y_uncertain_exp_u)\n",
    "                y_predict = c_model.predict(X_test)\n",
    "                c_accuracy = metrics.accuracy_score(y_test, y_predict)\n",
    "                c_prfsw = list(precision_recall_fscore_support(y_test, y_predict, average='weighted'))\n",
    "                c_prfsmi = list(precision_recall_fscore_support(y_test, y_predict, average='micro'))\n",
    "\n",
    "                l1 = [md[1], algorithm, batch_size, best_amount, len(\n",
    "                    X_uncertain_exp_u), c_accuracy]\n",
    "                l1.extend(c_prfsw)\n",
    "                l1.extend(c_prfsmi)\n",
    "\n",
    "\n",
    "                big_results.append(l1)\n",
    "                big_confusion.append(metrics.confusion_matrix(y_test, c_model.predict(X_test)))\n",
    "                #print(md[1], ',', algorithm, ',', batch_size, ',', best_amount, ',', len(X_uncertain_exp_u), ',', outp_algorithm, ',', c_accuracy)\n",
    "    del mdl\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame(big_results)\n",
    "res_df.to_csv(\"results/big_results3.csv\", index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[301  69]\n",
      "  [ 65 216]]\n",
      "\n",
      " [[326  44]\n",
      "  [ 69 212]]\n",
      "\n",
      " [[239 131]\n",
      "  [ 65 216]]\n",
      "\n",
      " [[313  57]\n",
      "  [ 59 222]]\n",
      "\n",
      " [[298  72]\n",
      "  [ 74 207]]\n",
      "\n",
      " [[314  56]\n",
      "  [ 58 223]]\n",
      "\n",
      " [[249 121]\n",
      "  [ 76 205]]\n",
      "\n",
      " [[312  58]\n",
      "  [ 61 220]]]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(big_confusion))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "25034407fed5d681614dac11a1c0537e8cb49e3a8883c071303eea01322943d9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
