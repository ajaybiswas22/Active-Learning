{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\AJAY\n",
      "[nltk_data]     BISWAS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import fasttext\n",
    "import pandas as pd\n",
    "import random\n",
    "import itertools\n",
    "from resources.tokTT import CommentTokenizer as CT\n",
    "from resources.basicIO import InputOutput as IO\n",
    "from resources.basicIO import InputOutput as IO\n",
    "from resources.filterLang import FilterLanguage as FL\n",
    "from resources.fasttext_transformer import classify\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import fasttext\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "from scipy.spatial import distance\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "#from sklearn.linear_model import LassoLars\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import copy\n",
    "import scipy\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import validation_curve\n",
    "import gc\n",
    "from sklearn.metrics import DetCurveDisplay, RocCurveDisplay\n",
    "from resources.expansion import Expand_U\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = pd.read_csv('datasets/random_sample_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_comment</th>\n",
       "      <th>tokenized_comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4477</th>\n",
       "      <td>all the flags are sikh religious flags. they d...</td>\n",
       "      <td>all the flag be sikh religious flag they do no...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4478</th>\n",
       "      <td>Salute the farmers</td>\n",
       "      <td>salute the farmer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4479</th>\n",
       "      <td>Let's also remember how many farmers lost thei...</td>\n",
       "      <td>let be also remember how many farmer lose thei...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4480</th>\n",
       "      <td>ABP news you guys dont have heart 8 farmers go...</td>\n",
       "      <td>abp news you guy dont have heart 8 farmer get ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4481</th>\n",
       "      <td>These new laws actually give the power to the ...</td>\n",
       "      <td>these new law actually give the power to the f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            raw_comment  \\\n",
       "4477  all the flags are sikh religious flags. they d...   \n",
       "4478                                 Salute the farmers   \n",
       "4479  Let's also remember how many farmers lost thei...   \n",
       "4480  ABP news you guys dont have heart 8 farmers go...   \n",
       "4481  These new laws actually give the power to the ...   \n",
       "\n",
       "                                      tokenized_comment  label  \n",
       "4477  all the flag be sikh religious flag they do no...      1  \n",
       "4478                                  salute the farmer      0  \n",
       "4479  let be also remember how many farmer lose thei...      0  \n",
       "4480  abp news you guy dont have heart 8 farmer get ...      0  \n",
       "4481  these new law actually give the power to the f...      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove unnecessary comments\n",
    "df_sample = df_sample.drop([x for x in range(len(df_sample)) if int(\n",
    "    df_sample['label'][x]) != 0 and int(df_sample['label'][x]) != 1])\n",
    "\n",
    "df_sample.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = df_sample['tokenized_comment']\n",
    "labels = df_sample['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weak Supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram_count(sentence: str, n_gram_phrase: str):\n",
    "    return sentence.count(n_gram_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def IDF(corpus, unique_words):\n",
    "   idf_dict = {}\n",
    "   N = len(corpus)\n",
    "   for i in unique_words:\n",
    "     count = 0\n",
    "     for sen in corpus:\n",
    "       if n_gram_count(sen,i) != 0:\n",
    "         count = count+1\n",
    "       idf_dict[i] = (math.log((1+N)/(count+1)))+1\n",
    "   return idf_dict\n",
    "\n",
    "\n",
    "def TF(sentence, n_gram_phrase, unique_words):\n",
    "    \"\"\"count of t in d / number of words in d\n",
    "\n",
    "       each phrase in unique_words is considered as a word\n",
    "    \"\"\"\n",
    "    freq = n_gram_count(sentence, n_gram_phrase)\n",
    "    total_words = len(sentence.split())\n",
    "    remove = 0\n",
    "    for word in unique_words:\n",
    "      freq_word = n_gram_count(sentence, word)\n",
    "      word_count = freq_word * len(word.split())\n",
    "      freq_word = word_count - freq_word\n",
    "      remove += freq_word\n",
    "    total_words -= remove\n",
    "    return freq/total_words\n",
    "\n",
    "def corpus_unique(corpus, n_gram_words):\n",
    "  \"\"\" returns the total unique words in corpus \"\"\"\n",
    "  all_sentences = (' '.join(corpus))\n",
    "  all_words = list(set(all_sentences.split()))\n",
    "  all_words.extend(n_gram_words)\n",
    "  \n",
    "  return list(set(all_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_words = ['wheat and rice','good for farmer','stand with','700 farmer',\n",
    "               'so call','so called','salute to','free market','rich farmer',\n",
    "                'we support','adani','ambani','khalistan','flag','victory',\n",
    "                'loss','corporates','middleman','middle man','khalistani']\n",
    "corpus_words = corpus_unique(list(comments), n_gram_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Feature Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = np.zeros((len(df_sample), len(n_gram_words)))\n",
    "\n",
    "comments_list = list(comments)\n",
    "\n",
    "labels_str = list(labels)\n",
    "labels_int = [int(i) for i in labels_str]\n",
    "\n",
    "for i in range(len(comments_list)):\n",
    "    for j in range(len(n_gram_words)):\n",
    "        tf_idf_ij = TF(comments_list[i], n_gram_words[j], corpus_words)\n",
    "        F[i][j] = tf_idf_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw = df_sample['tokenized_comment']\n",
    "y = df_sample['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "model_2 = fasttext.load_model(\"models/ft_unsupervised_N_2.bin\")\n",
    "model_3 = fasttext.load_model(\"models/ft_unsupervised_N_3.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3254, 320)\n"
     ]
    }
   ],
   "source": [
    "X_2 = [model_2[x] for x in X_raw]\n",
    "X_3 = [model_3[x] for x in X_raw]\n",
    "mdl = model_2\n",
    "X = np.hstack((X_2,F))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02240662  0.02617648 -0.06105003  0.02469208  0.03085181 -0.01056053\n",
      " -0.00797712 -0.00668243 -0.00024438  0.05027063 -0.01142706  0.0500012\n",
      " -0.00720274 -0.01717712 -0.0148936   0.03807749  0.06606516 -0.02020561\n",
      " -0.02553094 -0.03268506  0.00292853  0.01527047 -0.02881388  0.0093856\n",
      "  0.04100512 -0.04387803  0.01977714 -0.00414409 -0.01651457  0.01412005\n",
      " -0.02032061 -0.01771477  0.01930476  0.06880365 -0.02744946 -0.04024558\n",
      "  0.04712401 -0.03462878 -0.05357217 -0.02888196 -0.00759002 -0.03640257\n",
      "  0.01321779 -0.03102638  0.03284127 -0.08591734 -0.00766853 -0.00345979\n",
      "  0.03274981  0.01110621 -0.00768263 -0.0961987  -0.02196658  0.00567446\n",
      "  0.02912362  0.04259184 -0.0095796   0.04649972  0.06637276 -0.03027004\n",
      "  0.04684062 -0.02667232  0.03759938 -0.00304972  0.0258259  -0.07651334\n",
      " -0.03152761  0.00332509  0.02768348  0.02377841  0.03742892  0.07488141\n",
      "  0.01502203 -0.004222   -0.05046839 -0.02997868 -0.030106    0.06006132\n",
      " -0.0021668   0.02372488 -0.00618488 -0.02659548  0.02995998 -0.01573626\n",
      "  0.01169045 -0.00958646 -0.03843646  0.03216939 -0.08021453  0.00067704\n",
      "  0.02141128  0.03977616  0.00442617 -0.08014469 -0.03438907  0.02710271\n",
      " -0.01342988 -0.01444324  0.01863009  0.02914673  0.00687653 -0.04994835\n",
      " -0.01269426  0.06820214 -0.03067804 -0.0250412  -0.03484887  0.01380452\n",
      "  0.06144356 -0.01314828  0.018576    0.01452791  0.0037156   0.00348121\n",
      "  0.0059617   0.05727359  0.01108143 -0.00496179 -0.03315896  0.04293606\n",
      "  0.00050758 -0.06092293  0.05655087 -0.05763308 -0.01359874 -0.04154597\n",
      "  0.01383106 -0.03921517  0.00316998  0.07274609  0.06249092 -0.09163465\n",
      " -0.02501742 -0.04878594 -0.01533278  0.01798155 -0.06757444 -0.00722862\n",
      "  0.01521525 -0.02457366 -0.0388931  -0.00033195 -0.04672216 -0.03545194\n",
      " -0.00092743  0.00364626 -0.02881434  0.04888845  0.01186333 -0.01809075\n",
      " -0.04379524  0.02177681 -0.01562207  0.02135438 -0.00554303  0.03752316\n",
      " -0.00538062 -0.05758799  0.00928466  0.01637486  0.07790492  0.0355277\n",
      "  0.00980794 -0.00701007 -0.02267407  0.00444963  0.03486953 -0.0551868\n",
      " -0.01456652 -0.00708261  0.0420783   0.02535294 -0.00096366  0.03906647\n",
      " -0.03654895 -0.08000303  0.01389243 -0.04500728 -0.0541446  -0.03483732\n",
      " -0.04940077 -0.00398049 -0.05860757 -0.0365293  -0.00975493  0.0367218\n",
      "  0.02247671 -0.02972996 -0.02433763  0.00039368  0.01769802 -0.02083422\n",
      " -0.04844273 -0.03292207 -0.04115813 -0.02000131 -0.04486459  0.03594336\n",
      " -0.06339616 -0.04271754  0.02603442 -0.03999794  0.03562288  0.02033466\n",
      "  0.02650709  0.01890071 -0.04246581  0.0237943   0.03298495  0.01491072\n",
      " -0.05867777  0.03819019  0.00151042  0.05284961 -0.08584335  0.04197146\n",
      "  0.00936811  0.05890286 -0.01089071 -0.00424989  0.01754662  0.06218893\n",
      "  0.04430076  0.03784261 -0.0875669   0.03703257  0.0337501   0.03134042\n",
      " -0.01693431 -0.00214246 -0.02786415  0.05801887  0.02853077  0.02790228\n",
      " -0.03417019 -0.00475244 -0.04147505 -0.02111411  0.00068949 -0.02002156\n",
      " -0.03226277  0.00847306  0.02093879 -0.00171943  0.01479725 -0.00622169\n",
      "  0.04421763  0.01534246  0.03488427  0.06147058 -0.01058945 -0.01379428\n",
      "  0.0008636  -0.00216981 -0.01306022 -0.014747    0.09323305 -0.02640332\n",
      " -0.00903135  0.06935657  0.03703644 -0.00675184  0.09748125  0.05898255\n",
      "  0.01268411  0.01791469  0.02003571  0.03270092 -0.09904913 -0.02708031\n",
      " -0.01176671  0.06058867  0.04568371  0.00579734 -0.04165487 -0.05556449\n",
      "  0.07734372  0.0456222   0.03673278 -0.01920099  0.0439913  -0.05354281\n",
      "  0.03750924 -0.01878316  0.02963143 -0.07858928  0.03933384  0.04337464\n",
      " -0.0749854  -0.04021037 -0.0402938   0.00280233 -0.00034688  0.05720401\n",
      " -0.00068829 -0.06187205  0.06022723  0.016683   -0.00451026  0.02795011\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    df_sample['label'], test_size=0.2,\n",
    "                                                    random_state=37,\n",
    "                                                    stratify=df_sample['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 1000\n",
    "\n",
    "classifiers = {\n",
    "    \"SVM\": make_pipeline(StandardScaler(), svm.SVC(probability=True, kernel='rbf')),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=19),\n",
    "    \"LR\": make_pipeline(StandardScaler(), LogisticRegression(solver='lbfgs', penalty='l2', C=0.05, max_iter=10000, random_state=2)),\n",
    "    \"MLP\": make_pipeline(StandardScaler(), MLPClassifier(max_iter=1000, activation='logistic'))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_seed, X_expand, y_seed, y_expand = train_test_split(X_train,\n",
    "                                                      y_train, test_size=0.99,\n",
    "                                                      random_state=40,\n",
    "                                                      stratify=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('model ', 'algorithm, ', 'batch_size, ', 'best_amount, ', 'expanded_size, ', 'outp_algorithm, ', 'c_accuracy')\n",
    "\n",
    "algorithms = ['LR', 'SVM', 'KNN', 'MLP']\n",
    "batch_sizes = [20]\n",
    "best_amounts = [5]\n",
    "big_results = []\n",
    "big_confusion = []\n",
    "    \n",
    "X = np.hstack((X_2, X))\n",
    "\n",
    "for algorithm in algorithms:\n",
    "    alg = classifiers[algorithm]\n",
    "\n",
    "    for batch_size in batch_sizes:\n",
    "\n",
    "        for best_amount in best_amounts:\n",
    "\n",
    "            X_uncertain_exp_u, y_uncertain_exp_u, X_certain_exp_u, y_certain_exp_u = Expand_U(mdl,\n",
    "                                                                                                alg,\n",
    "                                                                                                list(\n",
    "                                                                                                    X_seed),\n",
    "                                                                                                list(\n",
    "                                                                                                    y_seed),\n",
    "                                                                                                list(\n",
    "                                                                                                    X_expand),\n",
    "                                                                                                list(\n",
    "                                                                                                    y_expand),\n",
    "                                                                                                batch_size,\n",
    "                                                                                                best_amount\n",
    "                                                                                                )\n",
    "\n",
    "            # classify\n",
    "            c_model = alg.fit(X_uncertain_exp_u, y_uncertain_exp_u)\n",
    "            y_predict = c_model.predict(X_test)\n",
    "            c_accuracy = metrics.accuracy_score(y_test, y_predict)\n",
    "            c_prfsw = list(precision_recall_fscore_support(\n",
    "                y_test, y_predict, average='weighted'))\n",
    "            c_prfsmi = list(precision_recall_fscore_support(\n",
    "                y_test, y_predict, average='micro'))\n",
    "\n",
    "            l1 = [mdl, algorithm, batch_size, best_amount, len(X_uncertain_exp_u), c_accuracy]\n",
    "            l1.extend(c_prfsw)\n",
    "            l1.extend(c_prfsmi)\n",
    "\n",
    "            big_results.append(l1)\n",
    "            big_confusion.append(metrics.confusion_matrix(\n",
    "                y_test, c_model.predict(X_test)))\n",
    "            #print(md[1], ',', algorithm, ',', batch_size, ',', best_amount, ',', len(X_uncertain_exp_u), ',', outp_algorithm, ',', c_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[<fasttext.FastText._FastText object at 0x00000241203E6B20>, 'LR', 20, 5, 666, 0.7112135176651305, 0.7098625098523159, 0.7112135176651305, 0.7102056808543334, None, 0.7112135176651305, 0.7112135176651305, 0.7112135176651304, None], [<fasttext.FastText._FastText object at 0x00000241203E6B20>, 'SVM', 20, 5, 666, 0.7265745007680492, 0.7282775147126072, 0.7265745007680492, 0.727179100169996, None, 0.7265745007680492, 0.7265745007680492, 0.7265745007680491, None], [<fasttext.FastText._FastText object at 0x00000241203E6B20>, 'KNN', 20, 5, 666, 0.6728110599078341, 0.6791301001728643, 0.6728110599078341, 0.6742484154313274, None, 0.6728110599078341, 0.6728110599078341, 0.6728110599078341, None], [<fasttext.FastText._FastText object at 0x00000241203E6B20>, 'MLP', 20, 5, 666, 0.7434715821812596, 0.7446413293346944, 0.7434715821812596, 0.7439140834230619, None, 0.7434715821812596, 0.7434715821812596, 0.7434715821812596, None]]\n"
     ]
    }
   ],
   "source": [
    "print(big_results)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "25034407fed5d681614dac11a1c0537e8cb49e3a8883c071303eea01322943d9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
