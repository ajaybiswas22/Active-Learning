{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\AJAY\n",
      "[nltk_data]     BISWAS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import fasttext\n",
    "import pandas as pd\n",
    "import random\n",
    "import itertools\n",
    "from resources.tokTT import CommentTokenizer as CT\n",
    "from resources.basicIO import InputOutput as IO\n",
    "from resources.basicIO import InputOutput as IO\n",
    "from resources.filterLang import FilterLanguage as FL\n",
    "from resources.fasttext_transformer import classify\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import fasttext\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "from scipy.spatial import distance\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "#from sklearn.linear_model import LassoLars\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import copy\n",
    "import scipy\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import validation_curve\n",
    "import gc\n",
    "from sklearn.metrics import DetCurveDisplay, RocCurveDisplay\n",
    "from resources.expansion import Expand_U\n",
    "from resources.expansion import Expand_UW\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "model_2 = fasttext.load_model(\"models/ft_unsupervised_N_2.bin\")\n",
    "model_3 = fasttext.load_model(\"models/ft_unsupervised_N_3.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = pd.read_csv('datasets/random_sample_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_comment</th>\n",
       "      <th>tokenized_comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4477</th>\n",
       "      <td>all the flags are sikh religious flags. they d...</td>\n",
       "      <td>all the flag be sikh religious flag they do no...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4478</th>\n",
       "      <td>Salute the farmers</td>\n",
       "      <td>salute the farmer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4479</th>\n",
       "      <td>Let's also remember how many farmers lost thei...</td>\n",
       "      <td>let be also remember how many farmer lose thei...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4480</th>\n",
       "      <td>ABP news you guys dont have heart 8 farmers go...</td>\n",
       "      <td>abp news you guy dont have heart 8 farmer get ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4481</th>\n",
       "      <td>These new laws actually give the power to the ...</td>\n",
       "      <td>these new law actually give the power to the f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            raw_comment  \\\n",
       "4477  all the flags are sikh religious flags. they d...   \n",
       "4478                                 Salute the farmers   \n",
       "4479  Let's also remember how many farmers lost thei...   \n",
       "4480  ABP news you guys dont have heart 8 farmers go...   \n",
       "4481  These new laws actually give the power to the ...   \n",
       "\n",
       "                                      tokenized_comment  label  \n",
       "4477  all the flag be sikh religious flag they do no...      1  \n",
       "4478                                  salute the farmer      0  \n",
       "4479  let be also remember how many farmer lose thei...      0  \n",
       "4480  abp news you guy dont have heart 8 farmer get ...      0  \n",
       "4481  these new law actually give the power to the f...      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove unnecessary comments\n",
    "df_sample = df_sample.drop([x for x in range(len(df_sample)) if int(\n",
    "    df_sample['label'][x]) != 0 and int(df_sample['label'][x]) != 1])\n",
    "\n",
    "df_sample.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = df_sample['tokenized_comment']\n",
    "labels = df_sample['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weak Supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram_count(sentence: str, n_gram_phrase: str):\n",
    "    return sentence.count(n_gram_phrase)\n",
    "\n",
    "\n",
    "def IDF(corpus, unique_words):\n",
    "   idf_dict = {}\n",
    "   N = len(corpus)\n",
    "   for i in unique_words:\n",
    "     count = 0\n",
    "     for sen in corpus:\n",
    "       if n_gram_count(sen,i) != 0:\n",
    "         count = count+1\n",
    "       idf_dict[i] = (math.log((1+N)/(count+1)))+1\n",
    "   return idf_dict\n",
    "\n",
    "\n",
    "def TF(sentence, n_gram_phrase, unique_words):\n",
    "    \"\"\"count of t in d / number of words in d\n",
    "\n",
    "       each phrase in unique_words is considered as a word\n",
    "    \"\"\"\n",
    "    freq = n_gram_count(sentence, n_gram_phrase)\n",
    "    total_words = len(sentence.split())\n",
    "    remove = 0\n",
    "    for word in unique_words:\n",
    "      freq_word = n_gram_count(sentence, word)\n",
    "      word_count = freq_word * len(word.split())\n",
    "      freq_word = word_count - freq_word\n",
    "      remove += freq_word\n",
    "    total_words -= remove\n",
    "    return freq/total_words\n",
    "\n",
    "def corpus_unique(corpus, n_gram_words):\n",
    "  \"\"\" returns the total unique words in corpus \"\"\"\n",
    "  all_sentences = (' '.join(corpus))\n",
    "  all_words = list(set(all_sentences.split()))\n",
    "  all_words.extend(n_gram_words)\n",
    "  \n",
    "  return list(set(all_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weak Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_words = ['wheat and rice','good for farmer','stand with','700 farmer',\n",
    "               'so call','so called','salute to','free market','rich farmer',\n",
    "                'we support','adani','ambani','khalistan','flag','victory',\n",
    "                'loss','corporates','middleman','middle man','khalistani','godi',\n",
    "                'jai jawan','jai kisan','no food','election','suicide','sikh',\n",
    "                'farmer from punjab','monopoly','only 6','win','other state']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Feature Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WeakFeatures(lines, n_gram_words):\n",
    "\n",
    "    F = np.zeros((len(lines), len(n_gram_words)))\n",
    "\n",
    "    comments_list = list(lines)\n",
    "\n",
    "    IDF_dict = IDF(lines, n_gram_words)\n",
    "    corpus_words = corpus_unique(comments_list, n_gram_words)\n",
    "\n",
    "    for i in range(len(comments_list)):\n",
    "        for j in range(len(n_gram_words)):\n",
    "            if n_gram_count(comments_list[i], n_gram_words[j]) !=0:\n",
    "                F[i][j] = 1\n",
    "            else:\n",
    "                F[i][j] = 0\n",
    "    return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_sample['tokenized_comment'],\n",
    "                                                    df_sample['label'], test_size=0.2,\n",
    "                                                    random_state=37,\n",
    "                                                    stratify=df_sample['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 1000\n",
    "\n",
    "classifiers = {\n",
    "    \"SVM\": make_pipeline(StandardScaler(), svm.SVC(probability=True, kernel='rbf')),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=19),\n",
    "    \"LR\": make_pipeline(StandardScaler(), LogisticRegression(solver='lbfgs', penalty='l2', C=0.05, max_iter=10000, random_state=2)),\n",
    "    \"MLP\": make_pipeline(StandardScaler(), MLPClassifier(max_iter=1000, activation='logistic'))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_seed, X_expand, y_seed, y_expand = train_test_split(X_train,\n",
    "                                                      y_train, test_size=0.99,\n",
    "                                                      random_state=40,\n",
    "                                                      stratify=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\AJAYBI~1\\AppData\\Local\\Temp/ipykernel_33152/268586663.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbest_amount\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbest_amounts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             X_uncertain_exp_u, y_uncertain_exp_u, X_certain_exp_u, y_certain_exp_u = Expand_UW(mdl,\n\u001b[0m\u001b[0;32m     20\u001b[0m                                                                                                 \u001b[0malg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                                                                                                 list(\n",
      "\u001b[1;32mc:\\Users\\AJAY BISWAS\\Documents\\GitHub\\Active-Learning\\farmers_protest_ajay_final\\resources\\expansion.py\u001b[0m in \u001b[0;36mExpand_UW\u001b[1;34m(model, algorithm, seed_set_tokenised, seed_set_label, expansion_tokenised, expansion_set_labels, batch_size, countMax, n_gram_words)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;31m# Weak supervision\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m         \u001b[0mF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWeakFeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed_TK\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_gram_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;31m# transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\AJAY BISWAS\\Documents\\GitHub\\Active-Learning\\farmers_protest_ajay_final\\resources\\expansion.py\u001b[0m in \u001b[0;36mWeakFeatures\u001b[1;34m(lines, n_gram_words)\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_gram_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[1;31m#F[i][j] = n_gram_count(comments_list[i], n_gram_words[j])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m             F[i][j] = TF(comments_list[i], n_gram_words[j],\n\u001b[0m\u001b[0;32m    127\u001b[0m                          corpus_words)*IDF_dict[n_gram_words[j]]\n\u001b[0;32m    128\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\AJAY BISWAS\\Documents\\GitHub\\Active-Learning\\farmers_protest_ajay_final\\resources\\expansion.py\u001b[0m in \u001b[0;36mTF\u001b[1;34m(sentence, n_gram_phrase, unique_words)\u001b[0m\n\u001b[0;32m     98\u001b[0m       \u001b[0mword_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfreq_word\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m       \u001b[0mfreq_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_count\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mfreq_word\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m       \u001b[0mremove\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mfreq_word\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m     \u001b[0mtotal_words\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mremove\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfreq\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#print('model ', 'algorithm, ', 'batch_size, ', 'best_amount, ', 'expanded_size, ', 'outp_algorithm, ', 'c_accuracy')\n",
    "\n",
    "algorithms = ['LR', 'SVM', 'KNN', 'MLP']\n",
    "batch_sizes = [20]\n",
    "best_amounts = [5]\n",
    "big_results = []\n",
    "big_confusion = []\n",
    "mdl = model_2\n",
    "    \n",
    "#X = np.hstack((X_2, X))\n",
    "\n",
    "for algorithm in algorithms:\n",
    "    alg = classifiers[algorithm]\n",
    "\n",
    "    for batch_size in batch_sizes:\n",
    "\n",
    "        for best_amount in best_amounts:\n",
    "\n",
    "            X_uncertain_exp_u, y_uncertain_exp_u, X_certain_exp_u, y_certain_exp_u = Expand_UW(mdl,\n",
    "                                                                                                alg,\n",
    "                                                                                                list(\n",
    "                                                                                                    X_seed),\n",
    "                                                                                                list(\n",
    "                                                                                                    y_seed),\n",
    "                                                                                                list(\n",
    "                                                                                                    X_expand),\n",
    "                                                                                                list(\n",
    "                                                                                                    y_expand),\n",
    "                                                                                                batch_size,\n",
    "                                                                                                best_amount,\n",
    "                                                                                                n_gram_words\n",
    "                                                                                                )\n",
    "\n",
    "            # Weak supervision\n",
    "            F = WeakFeatures(X_uncertain_exp_u, n_gram_words)\n",
    "\n",
    "            # transform\n",
    "            X = np.stack([np.mean([mdl[w] for w in text.split()], 0) for text in X_uncertain_exp_u])\n",
    "\n",
    "            # join them\n",
    "            XF = np.hstack((X,F))\n",
    "\n",
    "            # Weak supervision\n",
    "            FT = WeakFeatures(X_test, n_gram_words)\n",
    "\n",
    "            # transform\n",
    "            XT = np.stack([np.mean([mdl[w] for w in text.split()], 0)\n",
    "                         for text in X_test])\n",
    "\n",
    "            # join them\n",
    "            XFT = np.hstack((XT, FT))\n",
    "\n",
    "            c_model = alg.fit(XF, y_uncertain_exp_u)\n",
    "            y_predict = c_model.predict(XFT)\n",
    "            c_accuracy = metrics.accuracy_score(y_test, y_predict)\n",
    "            c_prfsw = list(precision_recall_fscore_support(y_test, y_predict,average='binary'))\n",
    "\n",
    "            l1 = ['N=2', algorithm, batch_size, best_amount, len(X_uncertain_exp_u), c_accuracy]\n",
    "            l1.extend(c_prfsw)\n",
    "\n",
    "            big_results.append(l1)\n",
    "            big_confusion.append(metrics.confusion_matrix(y_test, c_model.predict(XFT)))\n",
    "            #print(md[1], ',', algorithm, ',', batch_size, ',', best_amount, ',', len(X_uncertain_exp_u), ',', outp_algorithm, ',', c_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['N=2' 'LR' 20 5 666 0.7296466973886329 0.6937269372693727\n",
      "  0.6690391459074733 0.6811594202898551 None]\n",
      " ['N=2' 'SVM' 20 5 666 0.7864823348694316 0.7448275862068966\n",
      "  0.7686832740213523 0.756567425569177 None]\n",
      " ['N=2' 'KNN' 20 5 666 0.7127496159754224 0.6546052631578947\n",
      "  0.708185053380783 0.6803418803418804 None]\n",
      " ['N=2' 'MLP' 20 5 666 0.7342549923195084 0.6928571428571428\n",
      "  0.6903914590747331 0.6916221033868093 None]]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(big_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[287  83]\n",
      "  [ 93 188]]\n",
      "\n",
      " [[296  74]\n",
      "  [ 65 216]]\n",
      "\n",
      " [[265 105]\n",
      "  [ 82 199]]\n",
      "\n",
      " [[284  86]\n",
      "  [ 87 194]]]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(big_confusion))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "25034407fed5d681614dac11a1c0537e8cb49e3a8883c071303eea01322943d9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
