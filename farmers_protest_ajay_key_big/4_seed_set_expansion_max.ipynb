{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy import spatial\n",
    "from scipy.spatial import distance\n",
    "from resources.basicIO import InputOutput as IO\n",
    "from resources.filterLang import FilterLanguage as FL\n",
    "from resources.tokTT import CommentTokenizer as CT\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN(model, line, K):\n",
    "    return model.get_nearest_neighbors(line, k=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes 2 vectors a, b and returns the cosine similarity according \n",
    "# to the definition of the dot product\n",
    "def cos_sim(a, b):\n",
    "\tdot_product = np.dot(a, b)\n",
    "\tnorm_a = np.linalg.norm(a)\n",
    "\tnorm_b = np.linalg.norm(b)\n",
    "\treturn dot_product / (norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds similarity score between two lists\n",
    "def intersection_score(words, lst1, lst2, score_type):\n",
    "    # words contains all the words in the corpus\n",
    "    v1 = []\n",
    "    v2 = []\n",
    "\n",
    "    l1 = [x[1] for x in lst1]\n",
    "    l2 = [x[1] for x in lst2]\n",
    "\n",
    "    l10 = [x[0] for x in lst1]\n",
    "    l20 = [x[0] for x in lst2]\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        try:\n",
    "            v1.append(l10[l1.index(words[i])])\n",
    "        except:\n",
    "            v1.append(0)\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        try:\n",
    "            v2.append(l20[l2.index(words[i])])\n",
    "        except:\n",
    "            v2.append(0)\n",
    "\n",
    "    if(score_type == 'cosine_sim'):\n",
    "        return cos_sim(np.array(v1), np.array(v2))\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# load models\n",
    "model_N_2 = fasttext.load_model('models/ft_unsupervised_N_2.bin')\n",
    "model_N_3 = fasttext.load_model('models/ft_unsupervised_N_3.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Expansion Text and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading from 200th comments\n",
    "expansion_text = IO.load_csv_col('datasets/random_sample.csv','comment')\n",
    "expansion_text = expansion_text[0:100]\n",
    "expansion_text_labels = IO.load_csv_col('datasets/random_sample.csv','label')\n",
    "expansion_text_labels = list(map(str,map(int,expansion_text_labels[0:100])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Expansion text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "expansion_TK = [CT.tokenize(x) for x in expansion_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Seed Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_set_text = IO.load_text('datasets/seed_set.txt')\n",
    "seed_set_labels = IO.load_text('datasets/seed_set_labels.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Seed Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_set_TK = CT.cleaned('datasets/seed_set.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expand Seed Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expands seed set, seed set labels and NN_seed_set based on scores and also verifies with user labels \n",
    "# words: model.words, d1: NN_seed_set, l2: NN_exp_corpus_line, seed_set: seed_set, corpus_comment: exp_line, Y: labels\n",
    "def expand(words, d1, l2, g_seed_set, g_corpus_comment, g_corpus_comment_label, Y, score_type, to_check='F'):\n",
    "\n",
    "    scores = [intersection_score(words, i, l2, score_type) for i in d1]\n",
    "    maxpos = scores.index(max(scores))\n",
    "    # knn\n",
    "\n",
    "    try:\n",
    "        if(to_check == 'F'):\n",
    "            d1.insert(Y.index(Y[maxpos]), l2)\n",
    "            g_seed_set.insert(Y.index(Y[maxpos]), g_corpus_comment)\n",
    "            Y.insert(Y.index(Y[maxpos]), Y[maxpos])\n",
    "        else:\n",
    "            if(Y[maxpos] != g_corpus_comment_label):\n",
    "                d1.insert(Y.index(g_corpus_comment_label), l2)\n",
    "                g_seed_set.insert(Y.index(g_corpus_comment_label), g_corpus_comment)\n",
    "                Y.insert(Y.index(g_corpus_comment_label), g_corpus_comment_label)\n",
    "            else:\n",
    "                d1.insert(Y.index(Y[maxpos]), l2)\n",
    "                g_seed_set.insert(Y.index(Y[maxpos]), g_corpus_comment)\n",
    "                Y.insert(Y.index(Y[maxpos]), Y[maxpos])\n",
    "            \n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_seed_set(model,\n",
    "                    seed_set_text,\n",
    "                    seed_set_labels,\n",
    "                    seed_set_TK,\n",
    "                    expansion_text,\n",
    "                    expansion_TK, \n",
    "                    neighbors=50,\n",
    "                    expand_limit=100,\n",
    "                    score_type='cosine_sim'):\n",
    "    \n",
    "    # nearest neighbors of seed set\n",
    "    NN_seed_set = []\n",
    "    for comment in seed_set_TK:\n",
    "        NN_seed_set.append(NN(model,comment,neighbors))\n",
    "\n",
    "    # nearest neigbors for unlabeled corpus from random sample\n",
    "    NN_exp_corpus = []\n",
    "    for comment in expansion_TK:\n",
    "        NN_exp_corpus.append(NN(model,comment,neighbors))\n",
    "\n",
    "    # seed set to be expanded\n",
    "    seed_text_expanded = copy.deepcopy(seed_set_text)\n",
    "    Y_expanded = copy.deepcopy(seed_set_labels)\n",
    "    NN_seed_set_expanded = copy.deepcopy(NN_seed_set)\n",
    "\n",
    "    # expand by expand_limit\n",
    "    for i in range(expand_limit):\n",
    "        to_check='F'\n",
    "\n",
    "        expand(model.words,\n",
    "               NN_seed_set_expanded,\n",
    "               NN_exp_corpus[i],\n",
    "               seed_text_expanded,\n",
    "               expansion_text[i],\n",
    "               expansion_text_labels[i],\n",
    "               Y_expanded,\n",
    "               score_type,\n",
    "               to_check)\n",
    "\n",
    "    return seed_text_expanded, Y_expanded     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text_expanded_N_2,Y_expanded_N_2 = expand_seed_set(model_N_2,\n",
    "                                         seed_set_text,\n",
    "                                         seed_set_labels,\n",
    "                                         seed_set_TK,\n",
    "                                         expansion_text,\n",
    "                                         expansion_TK)\n",
    "\n",
    "IO.save_text('datasets_post/seed_set_expanded_N_2.txt', seed_text_expanded_N_2)\n",
    "IO.save_text('datasets_post/seed_set_expanded_labels_N_2.txt',\n",
    "             map(str, Y_expanded_N_2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text_expanded_N_3,Y_expanded_N_3 = expand_seed_set(model_N_3,\n",
    "                                         seed_set_text,\n",
    "                                         seed_set_labels,\n",
    "                                         seed_set_TK,\n",
    "                                         expansion_text,\n",
    "                                         expansion_TK)\n",
    "\n",
    "IO.save_text('datasets_post/seed_set_expanded_N_3.txt', seed_text_expanded_N_3)\n",
    "IO.save_text('datasets_post/seed_set_expanded_labels_N_3.txt',\n",
    "             map(str, Y_expanded_N_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.7982814908027649, 'isupportfarmers'), (0.7969151139259338, 'farmersprotest'), (0.763964056968689, 'wesupportfarmers'), (0.7365125417709351, 'farmerprotest'), (0.704119086265564, 'nofarmernofood'), (0.6894551515579224, 'supportfarmers'), (0.6165735125541687, 'nofarmersnofood'), (0.6114038825035095, 'protest'), (0.5750074982643127, 'support'), (0.5612239837646484, '</s>'), (0.5388291478157043, 'standwithfarmers'), (0.5361292958259583, 'tysm'), (0.5262243747711182, 'peacefuly'), (0.5213115215301514, 'jawan'), (0.5196169018745422, 'kisan'), (0.5150849223136902, 'thnks'), (0.5147648453712463, 'thank'), (0.5125710964202881, 'farmers'), (0.5114706754684448, 'cpi'), (0.5109272599220276, 'thanks')]\n",
      "\n",
      "[(0.7778640985488892, 'farmersprotest'), (0.7159777283668518, 'isupportfarmers'), (0.6801373958587646, 'wesupportfarmers'), (0.6648428440093994, 'nofarmernofood'), (0.6520518064498901, 'farmerprotest'), (0.6471043825149536, 'protest'), (0.6391842365264893, 'supportfarmers'), (0.5999649167060852, 'nofarmersnofood'), (0.5745700597763062, '</s>'), (0.572069525718689, 'mainstream'), (0.5444595813751221, 'farmers'), (0.536827564239502, 'support'), (0.5342505574226379, 'standwithfarmers'), (0.5341683626174927, 'tysm'), (0.5334911346435547, 'celebs'), (0.5197878479957581, 'istandwithfarmers'), (0.5117594599723816, 'haryanvi'), (0.5066937208175659, 'thanks'), (0.5062007308006287, 'janti'), (0.5005755424499512, 'unity')]\n",
      "\n",
      "[(0.606482207775116, '</s>'), (0.6020496487617493, 'good'), (0.5879366993904114, 'think'), (0.5776696801185608, 'actually'), (0.5634324550628662, 'going'), (0.560995876789093, 'not'), (0.5513339638710022, 'farmers'), (0.5446565747261047, 'hope'), (0.5444105863571167, 'see'), (0.5413100123405457, 'understand'), (0.5392200946807861, 'people'), (0.5389793515205383, 'laws'), (0.5360180139541626, 'much'), (0.5354668498039246, 'restricting'), (0.5330320596694946, 'government'), (0.5280550718307495, 'let'), (0.5245135426521301, 'themself'), (0.5243061184883118, 'enlightening'), (0.5230348706245422, 'would'), (0.5227733850479126, 'nevertheless')]\n"
     ]
    }
   ],
   "source": [
    "# Nearest Neighbors\n",
    "c1 = NN(model_N_2,'I support the farmers protest',20)\n",
    "c2 = NN(model_N_2,'I am against the farmers protest',20)\n",
    "c3 = NN(model_N_2, 'the farm bills are actually good the govt is doing right', 20)\n",
    "print(c1)\n",
    "print()\n",
    "print(c2)\n",
    "print()\n",
    "print(c3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7735561964283411"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cosine similarity\n",
    "intersection_score(model_N_2.words,c1,c2,'cosine_sim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09950162154847199"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersection_score(model_N_2.words,c2,c3,'cosine_sim')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee5cc6fef2d70a7e71ee3826687cbd150f18158e0b1eef11d4f4f92bb920e304"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
