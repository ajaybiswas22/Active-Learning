{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\AJAY\n",
      "[nltk_data]     BISWAS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\AJAY\n",
      "[nltk_data]     BISWAS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "import nltk.corpus\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.tokenize.stanford import StanfordTokenizer\n",
    "from nltk.util import bigrams, trigrams, ngrams\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "#from langdetect import detect\n",
    "#import sent2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence, to_lower=True,tknzr=TweetTokenizer()):\n",
    "    \"\"\"Arguments:\n",
    "        - tknzr: a tokenizer implementing the NLTK tokenizer interface\n",
    "        - sentence: a string to be tokenized\n",
    "        - to_lower: lowercasing or not\n",
    "    \"\"\"\n",
    "    sentence = sentence.strip()\n",
    "    sentence = ' '.join([format_token(x) for x in tknzr.tokenize(sentence)])\n",
    "    if to_lower:\n",
    "        sentence = sentence.lower()\n",
    "    # replace urls by <url>\n",
    "    sentence = re.sub(\n",
    "        '((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))', '<url>', sentence)\n",
    "    # replace @user268 by <user>\n",
    "    sentence = re.sub('(\\@[^\\s]+)', '<user>', sentence)\n",
    "    filter(lambda word: ' ' not in word, sentence)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def format_token(token):\n",
    "    \"\"\"\"\"\"\n",
    "    if token == '-LRB-':\n",
    "        token = '('\n",
    "    elif token == '-RRB-':\n",
    "        token = ')'\n",
    "    elif token == '-RSB-':\n",
    "        token = ']'\n",
    "    elif token == '-LSB-':\n",
    "        token = '['\n",
    "    elif token == '-LCB-':\n",
    "        token = '{'\n",
    "    elif token == '-RCB-':\n",
    "        token = '}'\n",
    "    return token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read comments into lines\n",
    "df_seed = pd.read_csv(\"train_set.csv\")\n",
    "df_corpus = pd.read_csv(\"unlabeled_corpus.csv\")\n",
    "\n",
    "lines_seed = df_seed[\"comment\"]\n",
    "lines_un = df_corpus[\"Comment\"]\n",
    "\n",
    "labels_seed = df_seed[\"label\"].apply(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuations and stopwords by applying filteredString to each line\n",
    "# not required\n",
    "lines_seed = list(map(tokenize, lines_seed))\n",
    "lines_un = list(map(tokenize, lines_un))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as txt\n",
    "with open('model.txt', mode='wt', encoding='utf-8') as myfile:\n",
    "    myfile.write('\\n'.join(lines_seed))\n",
    "\n",
    "with open('corpus.txt', mode='wt', encoding='utf-8') as myfile:\n",
    "    myfile.write('\\n'.join(lines_un))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as txt with label\n",
    "lines_seed_sup = ['__label__'.join(x) for x in zip(lines_seed, labels_seed)]\n",
    "with open('model_supervised.txt', mode='wt', encoding='utf-8') as myfile:\n",
    "    myfile.write('\\n'.join(lines_seed_sup))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "381 , 376\n",
      "14626 , 14519\n"
     ]
    }
   ],
   "source": [
    "# check if number of lines in txt files matches with csv files\n",
    "no_model = sum(1 for line in open('model.txt'))\n",
    "no_corpus = sum(1 for line in open('corpus.txt'))\n",
    "print(no_model,\",\",len(lines_seed))\n",
    "print(no_corpus,\",\",len(lines_un))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2e918aaa81d99c652401bdd1a0c185581595fb477ac919641bd65261b5d7782a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
