{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\AJAY\n",
      "[nltk_data]     BISWAS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import string\n",
    "import fasttext\n",
    "import math\n",
    "import copy\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.tokenize.stanford import StanfordTokenizer\n",
    "from scipy import spatial\n",
    "from scipy.spatial import distance\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "CLEANR = re.compile('<.*?>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text, punct_list):\n",
    "    for punc in punct_list:\n",
    "        if punc in text:\n",
    "            text = text.replace(punc, ' ')\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def remove_html_tags(raw_html):\n",
    "    cleantext = re.sub(CLEANR, '', raw_html)\n",
    "    return cleantext\n",
    "\n",
    "def tokenize(sentence, to_lower=True, tknzr=TweetTokenizer()):\n",
    "    \"\"\"Arguments:\n",
    "        - tknzr: a tokenizer implementing the NLTK tokenizer interface\n",
    "        - sentence: a string to be tokenized\n",
    "        - to_lower: lowercasing or not\n",
    "    \"\"\"\n",
    "    sentence = sentence.strip()\n",
    "    sentence = ' '.join([format_token(x) for x in tknzr.tokenize(sentence)])\n",
    "    if to_lower:\n",
    "        sentence = sentence.lower()\n",
    "    # replace urls by <url>\n",
    "    sentence = re.sub(\n",
    "        '((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))', '', sentence)\n",
    "    # replace @user268 by <user>\n",
    "    sentence = re.sub('(\\@[^\\s]+)', '', sentence)\n",
    "\n",
    "    filter(lambda word: ' ' not in word, sentence)\n",
    "\n",
    "    #remove single letter words\n",
    "    sentence = ' '.join([w for w in sentence.split() if len(w) > 1])\n",
    "\n",
    "    sentence = remove_html_tags(sentence)\n",
    "    sentence = remove_punctuation(sentence, list(string.punctuation))\n",
    "    sentence = ' '.join([word for word in sentence.split()\n",
    "                        if word not in cachedStopWords])\n",
    "    return sentence\n",
    "\n",
    "def format_token(token):\n",
    "    \"\"\"\"\"\"\n",
    "    if token == '-LRB-':\n",
    "        token = '('\n",
    "    elif token == '-RRB-':\n",
    "        token = ')'\n",
    "    elif token == '-RSB-':\n",
    "        token = ']'\n",
    "    elif token == '-LSB-':\n",
    "        token = '['\n",
    "    elif token == '-LCB-':\n",
    "        token = '{'\n",
    "    elif token == '-RCB-':\n",
    "        token = '}'\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaned(filename):\n",
    "\n",
    "    # Load\n",
    "    text_file = open(filename, \"r\")\n",
    "    no_str = text_file.read()\n",
    "    text_file.close()\n",
    "    # make a list\n",
    "    lines = no_str.split(\"\\n\")\n",
    "\n",
    "    #cleanup\n",
    "    temp = []\n",
    "    for comment in lines:\n",
    "        tok_comment = tokenize(comment)\n",
    "        temp.append(tok_comment)\n",
    "    lines = temp\n",
    "\n",
    "    return lines\n",
    "\n",
    "def load_labels(filename):\n",
    "    # Load\n",
    "    text_file = open(filename, \"r\")\n",
    "    no_str = text_file.read()\n",
    "    text_file.close()\n",
    "    # make a list\n",
    "    lines = no_str.split(\"\\n\")\n",
    "    return list(map(int, lines))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastTextTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Convert texts into their mean fastText vectors \"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.stack([\n",
    "            np.mean([self.model[w] for w in text.split()], 0)\n",
    "            for text in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(small_model,predictor,lines,Y):\n",
    "    classifier = make_pipeline(\n",
    "        FastTextTransformer(model=small_model),\n",
    "        predictor\n",
    "    ).fit(\n",
    "        lines,\n",
    "        Y\n",
    "    )\n",
    "    return classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# load models\n",
    "model_N_2 = fasttext.load_model('fp_bigrams_unsupervised_N_2.bin')\n",
    "model_N_3 = fasttext.load_model('fp_bigrams_unsupervised_N_3.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load seed set\n",
    "seed_set = cleaned(\"seed_set.txt\")\n",
    "# Load seed Labels\n",
    "Y = load_labels(\"seed_set_labels.txt\")\n",
    "\n",
    "# Load expanded seed set\n",
    "seed_set_expanded_N_2 = cleaned(\"seed_set_expanded_N_2.txt\")\n",
    "Y_N_2 = load_labels(\"seed_set_expanded_labels_N_2.txt\")\n",
    "\n",
    "seed_set_expanded_N_3 = cleaned(\"seed_set_expanded_N_3.txt\")\n",
    "Y_N_3 = load_labels(\"seed_set_expanded_labels_N_3.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load testing set\n",
    "testing = cleaned(\"testing.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification\n",
    "classifier_N_2_seed_set = classify(model_N_2, LogisticRegression(), seed_set, Y)\n",
    "classifier_N_3_seed_set = classify(model_N_3,LogisticRegression(),seed_set, Y)\n",
    "classifier_N_2_expanded_set = classify(model_N_2,LogisticRegression(),seed_set_expanded_N_2, Y_N_2)\n",
    "classifier_N_3_expanded_set = classify(model_N_3,LogisticRegression(),seed_set_expanded_N_3, Y_N_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 2,\n",
       "       2, 0, 1, 1, 1, 2, 2, 2])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_N_2_seed_set.predict(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 2,\n",
       "       2, 1, 1, 1, 1, 2, 2, 2])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_N_3_seed_set.predict(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 2,\n",
       "       2, 0, 1, 1, 1, 2, 2, 2])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_N_2_expanded_set.predict(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,\n",
       "       2, 1, 1, 1, 1, 0, 2, 2])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_N_3_expanded_set.predict(testing)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee5cc6fef2d70a7e71ee3826687cbd150f18158e0b1eef11d4f4f92bb920e304"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
