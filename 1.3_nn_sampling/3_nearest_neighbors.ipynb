{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\AJAY\n",
      "[nltk_data]     BISWAS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import fasttext\n",
    "import math\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.tokenize.stanford import StanfordTokenizer\n",
    "from scipy import spatial\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "CLEANR = re.compile('<.*?>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text, punct_list):\n",
    "    for punc in punct_list:\n",
    "        if punc in text:\n",
    "            text = text.replace(punc, ' ')\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def remove_html_tags(raw_html):\n",
    "    cleantext = re.sub(CLEANR, '', raw_html)\n",
    "    return cleantext\n",
    "\n",
    "def tokenize(sentence, to_lower=True, tknzr=TweetTokenizer()):\n",
    "    \"\"\"Arguments:\n",
    "        - tknzr: a tokenizer implementing the NLTK tokenizer interface\n",
    "        - sentence: a string to be tokenized\n",
    "        - to_lower: lowercasing or not\n",
    "    \"\"\"\n",
    "    sentence = sentence.strip()\n",
    "    sentence = ' '.join([format_token(x) for x in tknzr.tokenize(sentence)])\n",
    "    if to_lower:\n",
    "        sentence = sentence.lower()\n",
    "    # replace urls by <url>\n",
    "    sentence = re.sub(\n",
    "        '((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))', '', sentence)\n",
    "    # replace @user268 by <user>\n",
    "    sentence = re.sub('(\\@[^\\s]+)', '', sentence)\n",
    "\n",
    "    filter(lambda word: ' ' not in word, sentence)\n",
    "\n",
    "    #remove single letter words\n",
    "    sentence = ' '.join([w for w in sentence.split() if len(w) > 1])\n",
    "\n",
    "    sentence = remove_html_tags(sentence)\n",
    "    sentence = remove_punctuation(sentence, list(string.punctuation))\n",
    "    sentence = ' '.join([word for word in sentence.split()\n",
    "                        if word not in cachedStopWords])\n",
    "    return sentence\n",
    "\n",
    "def format_token(token):\n",
    "    \"\"\"\"\"\"\n",
    "    if token == '-LRB-':\n",
    "        token = '('\n",
    "    elif token == '-RRB-':\n",
    "        token = ')'\n",
    "    elif token == '-RSB-':\n",
    "        token = ']'\n",
    "    elif token == '-LSB-':\n",
    "        token = '['\n",
    "    elif token == '-LCB-':\n",
    "        token = '{'\n",
    "    elif token == '-RCB-':\n",
    "        token = '}'\n",
    "    return token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN(model,line,K):\n",
    "    return model.get_nearest_neighbors(line, k=K)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]\n",
    "        y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    if(math.sqrt(sumxx*sumyy) == 0):\n",
    "        return 0\n",
    "    else:\n",
    "        return sumxy/math.sqrt(sumxx*sumyy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity (https://paulminogue.com/index.php/2019/09/29/introduction-to-cosine-similarity/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = fasttext.load_model('fp_bigrams_unsupervised.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load seed set\n",
    "text_file = open(\"seed_set.txt\", \"r\")\n",
    "no_str = text_file.read()\n",
    "text_file.close()\n",
    "# make a list\n",
    "seed_set = no_str.split(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corpus\n",
    "text_file2 = open(\"corpus.txt\", \"r\")\n",
    "no_str2 = text_file2.read()\n",
    "text_file2.close()\n",
    "# make a list\n",
    "corpus = no_str2.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Neighbor sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nearest neigbors for seed set, setting k = 20\n",
    "NN_seed_set = []\n",
    "for comment in seed_set:\n",
    "    tok_comment = tokenize(comment)\n",
    "    NN_seed_set.append(NN(model,tok_comment,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nearest neigbors for unlabeled corpus, setting k = 20\n",
    "NN_corpus = []\n",
    "for comment in corpus:\n",
    "    tok_comment = tokenize(comment)\n",
    "    NN_corpus.append(NN(model,tok_comment,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN Sampling by finding distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.8419418334960938, 'farmer'), (0.7533829808235168, 'bill'), (0.747085452079773, 'let'), (0.7365711331367493, 'win'), (0.7077261209487915, 'protesting'), (0.687737226486206, 'back'), (0.686690628528595, 'without'), (0.6720230579376221, 'mandi'), (0.6683642268180847, 'middle'), (0.6678685545921326, 'fighting'), (0.6671098470687866, 'ask'), (0.6629432439804077, 'face'), (0.6585047245025635, 'terrorists'), (0.6526790857315063, 'months'), (0.6507577300071716, 'cannot'), (0.6430784463882446, 'farm'), (0.6409282684326172, 'lol'), (0.6402221918106079, 'protest'), (0.6371271014213562, 'violence'), (0.636122465133667, 'take')]\n"
     ]
    }
   ],
   "source": [
    "print(NN_seed_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seed Set Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_score(lst1, lst2):\n",
    "    l1 = [x[1] for x in lst1]\n",
    "    l2 = [x[1] for x in lst2]\n",
    "\n",
    "    l10 = [x[0] for x in lst1]\n",
    "    l20 = [x[0] for x in lst2]\n",
    "\n",
    "\n",
    "    lst3 = [value for value in l1 if value in l2]\n",
    "\n",
    "    v1= []\n",
    "    v2 = []\n",
    "\n",
    "    for i in range(len(l1)):\n",
    "        for j in range(len(lst3)):\n",
    "            if(l1[i] == lst3[j]):\n",
    "                v1.append(l10[i])\n",
    "\n",
    "    for i in range(len(l2)):\n",
    "        for j in range(len(lst3)):\n",
    "            if(l2[i] == lst3[j]):\n",
    "                v2.append(l20[i])\n",
    "\n",
    "    return cosine_similarity(v1,v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_label(d1, l2, seed_set, corpus_comment, Y):\n",
    "\n",
    "    scores = [intersection_score(i, l2) for i in d1]\n",
    "    maxpos = scores.index(max(scores))\n",
    "    \n",
    "    if(Y[maxpos] == 0):\n",
    "        Y.append(0)\n",
    "        d1.append(l2)\n",
    "        seed_set.append(corpus_comment)\n",
    "    else:\n",
    "        Y.insert(0,1)\n",
    "        d1.insert(0,l2)\n",
    "        seed_set.insert(0,corpus_comment)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels for 10 seed set\n",
    "Y = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
    "\n",
    "seed_set_expanded = seed_set\n",
    "Y_expanded = Y\n",
    "NN_seed_set_expanded = NN_seed_set\n",
    "\n",
    "# expand by 100 comments\n",
    "for i in range(20): \n",
    "    expand_label(NN_seed_set_expanded,NN_corpus[i+1000],seed_set_expanded, corpus[i+1000], Y_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as txt\n",
    "with open('seed_set_expanded.txt', mode='wt', encoding='utf-8') as myfile:\n",
    "    myfile.write('\\n'.join(seed_set_expanded))\n",
    "\n",
    "Y_expanded_string = map(str, Y_expanded)\n",
    "with open('seed_set_expanded_labels.txt', mode='wt', encoding='utf-8') as myfile:\n",
    "    myfile.write('\\n'.join(Y_expanded_string))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee5cc6fef2d70a7e71ee3826687cbd150f18158e0b1eef11d4f4f92bb920e304"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
