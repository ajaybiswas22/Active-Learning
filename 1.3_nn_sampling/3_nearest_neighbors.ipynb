{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\AJAY\n",
      "[nltk_data]     BISWAS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import fasttext\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.tokenize.stanford import StanfordTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "cachedStopWords = stopwords.words(\"english\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text, punct_list):\n",
    "    for punc in punct_list:\n",
    "        if punc in text:\n",
    "            text = text.replace(punc, ' ')\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    import re\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "def tokenize(sentence, to_lower=True, tknzr=TweetTokenizer()):\n",
    "    \"\"\"Arguments:\n",
    "        - tknzr: a tokenizer implementing the NLTK tokenizer interface\n",
    "        - sentence: a string to be tokenized\n",
    "        - to_lower: lowercasing or not\n",
    "    \"\"\"\n",
    "    sentence = sentence.strip()\n",
    "    sentence = ' '.join([format_token(x) for x in tknzr.tokenize(sentence)])\n",
    "    if to_lower:\n",
    "        sentence = sentence.lower()\n",
    "    # replace urls by <url>\n",
    "    sentence = re.sub(\n",
    "        '((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))', '', sentence)\n",
    "    # replace @user268 by <user>\n",
    "    sentence = re.sub('(\\@[^\\s]+)', '', sentence)\n",
    "\n",
    "    filter(lambda word: ' ' not in word, sentence)\n",
    "\n",
    "    #remove single letter words\n",
    "    sentence = ' '.join([w for w in sentence.split() if len(w) > 1])\n",
    "\n",
    "    sentence = remove_html_tags(sentence)\n",
    "    regular_punct = list(string.punctuation)\n",
    "    sentence = remove_punctuation(sentence, regular_punct)\n",
    "    sentence = ' '.join([word for word in sentence.split()\n",
    "                        if word not in cachedStopWords])\n",
    "    return sentence\n",
    "\n",
    "def format_token(token):\n",
    "    \"\"\"\"\"\"\n",
    "    if token == '-LRB-':\n",
    "        token = '('\n",
    "    elif token == '-RRB-':\n",
    "        token = ')'\n",
    "    elif token == '-RSB-':\n",
    "        token = ']'\n",
    "    elif token == '-LSB-':\n",
    "        token = '['\n",
    "    elif token == '-LCB-':\n",
    "        token = '{'\n",
    "    elif token == '-RCB-':\n",
    "        token = '}'\n",
    "    return token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN(model,line,K):\n",
    "    return model.get_nearest_neighbors(line, k=K)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_similarity(feature_vec_1, feature_vec_2):\n",
    "    return cosine_similarity(feature_vec_1.reshape(1, -1), feature_vec_2.reshape(1, -1))[0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity (https://paulminogue.com/index.php/2019/09/29/introduction-to-cosine-similarity/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = fasttext.load_model('fp_bigrams_unsupervised.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load seed set\n",
    "text_file = open(\"seed_set.txt\", \"r\")\n",
    "no_str = text_file.read()\n",
    "text_file.close()\n",
    "# make a list\n",
    "seed_set = no_str.split(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corpus\n",
    "text_file2 = open(\"corpus.txt\", \"r\")\n",
    "no_str2 = text_file2.read()\n",
    "text_file2.close()\n",
    "# make a list\n",
    "corpus = no_str2.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels for 10 seed set\n",
    "Y = [1,1,1,1,1,0,0,0,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Neighbor sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nearest neigbors for seed set, setting k = 20\n",
    "NN_seed_set = []\n",
    "for comment in seed_set:\n",
    "    tok_comment = tokenize(comment)\n",
    "    NN_seed_set.append(NN(model,tok_comment,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nearest neigbors for unlabeled corpus, setting k = 20\n",
    "NN_corpus = []\n",
    "for comment in corpus:\n",
    "    tok_comment = tokenize(comment)\n",
    "    NN_corpus.append(NN(model,tok_comment,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN Sampling by finding distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9190638065338135, 'protest'), (0.9121805429458618, 'protests'), (0.8388497829437256, 'farmers'), (0.7637169361114502, 'stop'), (0.7555079460144043, 'propaganda'), (0.7416914701461792, 'protesting'), (0.7286685705184937, 'back'), (0.7275574207305908, 'bill'), (0.7145411968231201, 'farmer'), (0.6960648894309998, 'still'), (0.6925775408744812, 'farm'), (0.6840285062789917, 'law'), (0.680992603302002, 'live'), (0.6618435382843018, '</s>'), (0.6589888334274292, 'jai'), (0.6566498875617981, 'rich'), (0.652950644493103, 'modi'), (0.6461869478225708, 'ram'), (0.6447934508323669, 'name'), (0.6335991621017456, 'terrorists')]\n"
     ]
    }
   ],
   "source": [
    "print(NN_seed_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9034309387207031, 'without'), (0.8403633236885071, 'months'), (0.8322609663009644, 'way'), (0.8184826374053955, 'mandi'), (0.8161060810089111, 'need'), (0.8135474324226379, 'another'), (0.8135462403297424, 'farmers'), (0.7662057876586914, 'middle'), (0.7607018947601318, 'win'), (0.7604532241821289, 'still'), (0.7437332272529602, 'would'), (0.7283453345298767, 'bill'), (0.7243574261665344, 'rights'), (0.7215253710746765, 'fighting'), (0.7155255675315857, 'government'), (0.708303689956665, 'support'), (0.7049741744995117, 'lol'), (0.7008495330810547, 'hope'), (0.6988953948020935, 'due'), (0.6880658864974976, 'man')]\n"
     ]
    }
   ],
   "source": [
    "print(NN_corpus[0])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee5cc6fef2d70a7e71ee3826687cbd150f18158e0b1eef11d4f4f92bb920e304"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
