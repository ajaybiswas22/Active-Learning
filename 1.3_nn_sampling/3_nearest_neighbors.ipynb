{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\AJAY\n",
      "[nltk_data]     BISWAS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import fasttext\n",
    "import math\n",
    "import copy\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.tokenize.stanford import StanfordTokenizer\n",
    "from scipy import spatial\n",
    "from scipy.spatial import distance\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "CLEANR = re.compile('<.*?>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text, punct_list):\n",
    "    for punc in punct_list:\n",
    "        if punc in text:\n",
    "            text = text.replace(punc, ' ')\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def remove_html_tags(raw_html):\n",
    "    cleantext = re.sub(CLEANR, '', raw_html)\n",
    "    return cleantext\n",
    "\n",
    "def tokenize(sentence, to_lower=True, tknzr=TweetTokenizer()):\n",
    "    \"\"\"Arguments:\n",
    "        - tknzr: a tokenizer implementing the NLTK tokenizer interface\n",
    "        - sentence: a string to be tokenized\n",
    "        - to_lower: lowercasing or not\n",
    "    \"\"\"\n",
    "    sentence = sentence.strip()\n",
    "    sentence = ' '.join([format_token(x) for x in tknzr.tokenize(sentence)])\n",
    "    if to_lower:\n",
    "        sentence = sentence.lower()\n",
    "    # replace urls by <url>\n",
    "    sentence = re.sub(\n",
    "        '((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))', '', sentence)\n",
    "    # replace @user268 by <user>\n",
    "    sentence = re.sub('(\\@[^\\s]+)', '', sentence)\n",
    "\n",
    "    filter(lambda word: ' ' not in word, sentence)\n",
    "\n",
    "    #remove single letter words\n",
    "    sentence = ' '.join([w for w in sentence.split() if len(w) > 1])\n",
    "\n",
    "    sentence = remove_html_tags(sentence)\n",
    "    sentence = remove_punctuation(sentence, list(string.punctuation))\n",
    "    sentence = ' '.join([word for word in sentence.split()\n",
    "                        if word not in cachedStopWords])\n",
    "    return sentence\n",
    "\n",
    "def format_token(token):\n",
    "    \"\"\"\"\"\"\n",
    "    if token == '-LRB-':\n",
    "        token = '('\n",
    "    elif token == '-RRB-':\n",
    "        token = ')'\n",
    "    elif token == '-RSB-':\n",
    "        token = ']'\n",
    "    elif token == '-LSB-':\n",
    "        token = '['\n",
    "    elif token == '-LCB-':\n",
    "        token = '{'\n",
    "    elif token == '-RCB-':\n",
    "        token = '}'\n",
    "    return token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN(model,line,K):\n",
    "    return model.get_nearest_neighbors(line, k=K)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(a, b):\n",
    "\t\"\"\"Takes 2 vectors a, b and returns the cosine similarity according \n",
    "\tto the definition of the dot product\n",
    "\t\"\"\"\n",
    "\tdot_product = np.dot(a, b)\n",
    "\tnorm_a = np.linalg.norm(a)\n",
    "\tnorm_b = np.linalg.norm(b)\n",
    "\t\n",
    "\treturn dot_product / (norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = fasttext.load_model('fp_bigrams_unsupervised_N_3.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "# Load seed set\n",
    "text_file = open(\"seed_set.txt\", \"r\")\n",
    "no_str = text_file.read()\n",
    "text_file.close()\n",
    "# make a list\n",
    "seed_set = no_str.split(\"\\n\")\n",
    "\n",
    "# Load Labels\n",
    "df_seed = pd.read_csv(\"seed_set.csv\")\n",
    "Y = df_seed[\"label\"].tolist()\n",
    "print(Y)\n",
    "\n",
    "# keep a copy\n",
    "seed_set_copy = copy.deepcopy(seed_set)\n",
    "#cleanup\n",
    "temp = []\n",
    "for comment in seed_set:\n",
    "    tok_comment = tokenize(comment)\n",
    "    temp.append(tok_comment)\n",
    "seed_set = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corpus\n",
    "text_file2 = open(\"corpus.txt\", \"r\")\n",
    "no_str2 = text_file2.read()\n",
    "text_file2.close()\n",
    "# make a list\n",
    "corpus = no_str2.split(\"\\n\")\n",
    "\n",
    "# keep a copy\n",
    "corpus_copy = copy.deepcopy(corpus)\n",
    "# cleanup\n",
    "temp = []\n",
    "for comment in corpus:\n",
    "    tok_comment = tokenize(comment)\n",
    "    temp.append(tok_comment)\n",
    "corpus = temp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Neighbor sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nearest neigbors for seed set, setting k = 20\n",
    "NN_seed_set = []\n",
    "for comment in seed_set:\n",
    "    NN_seed_set.append(NN(model,comment,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nearest neigbors for unlabeled corpus, setting k = 20\n",
    "NN_corpus = []\n",
    "for comment in corpus:\n",
    "    NN_corpus.append(NN(model,comment,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN Sampling by finding distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_score(words, lst1, lst2):\n",
    "\n",
    "    # words contains all the words in the corpus\n",
    "    v1 = []\n",
    "    v2 = []\n",
    "\n",
    "    l1 = [x[1] for x in lst1]\n",
    "    l2 = [x[1] for x in lst2]\n",
    "\n",
    "    l10 = [x[0] for x in lst1]\n",
    "    l20 = [x[0] for x in lst2]\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        try:\n",
    "            v1.append(l10[l1.index(words[i])])\n",
    "        except:\n",
    "            v1.append(0)\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        try:\n",
    "            v2.append(l20[l2.index(words[i])])\n",
    "        except:\n",
    "            v2.append(0)\n",
    "    \n",
    "    return cos_sim(np.array(v1), np.array(v2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kisan andolan propagation necessary take matter poor people suffering kisan andolan lost livelihood\n",
      "[(0.6410973072052002, 'livelihood'), (0.5635316371917725, 'suffering'), (0.5284897685050964, 'protest'), (0.5018327832221985, '</s>'), (0.4985179603099823, 'farmers'), (0.4971248507499695, 'people'), (0.4915183484554291, 'naxal'), (0.49038076400756836, 'pandemic'), (0.48261135816574097, 'government'), (0.47489815950393677, 'harvesting'), (0.4733964800834656, 'mobs'), (0.47337886691093445, 'fault'), (0.47262337803840637, 'andolan'), (0.47045546770095825, 'silent'), (0.4702516198158264, 'leader'), (0.4669043719768524, 'proposed'), (0.4637429714202881, 'extreme'), (0.46342751383781433, 'trouble'), (0.46299922466278076, 'popular'), (0.46165594458580017, 'asmr')]\n",
      "choukidar chor jasoos 2024 choukidar chor gol\n",
      "0.04541796042126951\n"
     ]
    }
   ],
   "source": [
    "print(seed_set[0])\n",
    "print(NN_seed_set[0])\n",
    "print(corpus[0])\n",
    "print(intersection_score(model.words, NN_seed_set[0],NN_corpus[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words: model.words, d1: NN_seed_set, l2: NN_corpus_line, g_seed_set: seed_set, g_corpus_comment: corpus_line, Y:labels\n",
    "def expand_label(words,d1, l2, g_seed_set, g_corpus_comment, Y):\n",
    "\n",
    "    scores = [intersection_score(words,i, l2) for i in d1]\n",
    "    maxpos = scores.index(max(scores))\n",
    "\n",
    "    try:\n",
    "        if(Y[maxpos] == 0):\n",
    "            Y.insert(0,0)\n",
    "            d1.insert(0,l2)\n",
    "            g_seed_set.insert(0,g_corpus_comment)\n",
    "        elif(Y[maxpos] == 2):\n",
    "            Y.append(2)\n",
    "            d1.append(l2)\n",
    "            g_seed_set.append(g_corpus_comment)\n",
    "        elif(Y[maxpos] == 1):\n",
    "            d1.insert(Y.index(1),l2)\n",
    "            g_seed_set.insert(Y.index(1),g_corpus_comment)\n",
    "            Y.insert(Y.index(1),1)\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand seed set along with labels\n",
    "Y_expanded = Y\n",
    "NN_seed_set_expanded = NN_seed_set\n",
    "\n",
    "# expand by 100 comments\n",
    "for i in range(100): \n",
    "    expand_label(model.words,NN_seed_set_expanded,NN_corpus[i+1000],seed_set_copy, corpus_copy[i+1000], Y_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as txt\n",
    "with open('seed_set_expanded_N_3.txt', mode='wt', encoding='utf-8') as myfile:\n",
    "    myfile.write('\\n'.join(seed_set_copy))\n",
    "\n",
    "Y_expanded_string = map(str, Y_expanded)\n",
    "with open('seed_set_expanded_labels_N_3.txt', mode='wt', encoding='utf-8') as myfile:\n",
    "    myfile.write('\\n'.join(Y_expanded_string))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee5cc6fef2d70a7e71ee3826687cbd150f18158e0b1eef11d4f4f92bb920e304"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
