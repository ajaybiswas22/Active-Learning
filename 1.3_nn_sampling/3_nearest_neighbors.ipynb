{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.tokenize.stanford import StanfordTokenizer\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tknzr, sentence, to_lower=True):\n",
    "    \"\"\"Arguments:\n",
    "        - tknzr: a tokenizer implementing the NLTK tokenizer interface\n",
    "        - sentence: a string to be tokenized\n",
    "        - to_lower: lowercasing or not\n",
    "    \"\"\"\n",
    "    sentence = sentence.strip()\n",
    "    sentence = ' '.join([format_token(x) for x in tknzr.tokenize(sentence)])\n",
    "    if to_lower:\n",
    "        sentence = sentence.lower()\n",
    "    sentence = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))','<url>',sentence) #replace urls by <url>\n",
    "    sentence = re.sub('(\\@[^\\s]+)','<user>',sentence) #replace @user268 by <user>\n",
    "    filter(lambda word: ' ' not in word, sentence)\n",
    "    return sentence\n",
    "\n",
    "def format_token(token):\n",
    "    \"\"\"\"\"\"\n",
    "    if token == '-LRB-':\n",
    "        token = '('\n",
    "    elif token == '-RRB-':\n",
    "        token = ')'\n",
    "    elif token == '-RSB-':\n",
    "        token = ']'\n",
    "    elif token == '-LSB-':\n",
    "        token = '['\n",
    "    elif token == '-LCB-':\n",
    "        token = '{'\n",
    "    elif token == '-RCB-':\n",
    "        token = '}'\n",
    "    return token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN(model,line,K):\n",
    "    return model.get_nearest_neighbors(line, k=K)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(v1,v2):\n",
    "    return distance.cosine(v1,v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = fasttext.load_model('fp_bigrams_unsupervised.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load seed set\n",
    "text_file = open(\"seed_set.txt\", \"r\")\n",
    "no_str = text_file.read()\n",
    "text_file.close()\n",
    "# make a list\n",
    "seed_set = no_str.split(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corpus\n",
    "text_file2 = open(\"corpus.txt\", \"r\")\n",
    "no_str2 = text_file2.read()\n",
    "text_file2.close()\n",
    "# make a list\n",
    "corpus = no_str2.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels for 10 seed set\n",
    "Y = [1,1,1,1,1,0,0,0,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Neighbor sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nearest neigbors for seed set, setting k = 20\n",
    "NN_seed_set = []\n",
    "for comment in seed_set:\n",
    "    tok_comment = tokenize(TweetTokenizer(),comment)\n",
    "    NN_seed_set.append(NN(model,tok_comment,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nearest neigbors for unlabeled corpus, setting k = 20\n",
    "NN_corpus = []\n",
    "for comment in corpus:\n",
    "    tok_comment = tokenize(TweetTokenizer(), comment)\n",
    "    NN_corpus.append(NN(model,tok_comment,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN Sampling by finding distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.7868461012840271, 'farmer'), (0.7597593665122986, 'protest'), (0.7351622581481934, 'protests'), (0.6815500855445862, 'should'), (0.648932933807373, 'anti'), (0.634634256362915, 'mandi'), (0.6068609356880188, 'bill'), (0.6053192019462585, 'ram'), (0.6050131916999817, 'without'), (0.5900097489356995, 'farmers'), (0.5886179208755493, 'live'), (0.5833055973052979, 'by'), (0.5544314384460449, 'these'), (0.5535263419151306, 'if'), (0.5504714250564575, 'law'), (0.5485056638717651, 'doing'), (0.5484163761138916, 'rich'), (0.5476019382476807, 'there'), (0.5462289452552795, 'let'), (0.528472900390625, 'jai')]\n"
     ]
    }
   ],
   "source": [
    "print(NN_seed_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.8319745063781738, 'without'), (0.6469683647155762, 'middle'), (0.6072287559509277, 'mandi'), (0.6034027338027954, 'should'), (0.5961973071098328, 'such'), (0.5930907726287842, 'anti'), (0.5862780809402466, 'let'), (0.5781086087226868, 'ravish'), (0.5633074641227722, 'out'), (0.5371558666229248, 'there'), (0.5352479219436646, 'way'), (0.5222377181053162, 'help'), (0.5206180214881897, 'farmer'), (0.5202722549438477, 'need'), (0.5191149115562439, 'these'), (0.51839280128479, 'shameless'), (0.5153034925460815, 'bill'), (0.5109532475471497, 'all'), (0.508699357509613, 'many'), (0.5083184838294983, 'for')]\n"
     ]
    }
   ],
   "source": [
    "print(NN_corpus[0])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee5cc6fef2d70a7e71ee3826687cbd150f18158e0b1eef11d4f4f92bb920e304"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
