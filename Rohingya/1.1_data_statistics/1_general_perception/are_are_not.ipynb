{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\AJAY\n",
      "[nltk_data]     BISWAS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\AJAY\n",
      "[nltk_data]     BISWAS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "import nltk.corpus\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import bigrams, trigrams, ngrams\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Username</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Video ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CuriousLearner</td>\n",
       "      <td>What about Kashmiri Hindus. Never such mercy f...</td>\n",
       "      <td>0</td>\n",
       "      <td>F25qrkboV3o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am me</td>\n",
       "      <td>10 crore illigal Rohingya in india</td>\n",
       "      <td>0</td>\n",
       "      <td>F25qrkboV3o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am me</td>\n",
       "      <td>10 crore Rohingya muslims in india</td>\n",
       "      <td>1</td>\n",
       "      <td>F25qrkboV3o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OB</td>\n",
       "      <td>What happened in Bangladesh before 1980 to cau...</td>\n",
       "      <td>0</td>\n",
       "      <td>F25qrkboV3o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Khadar Osman</td>\n",
       "      <td>I really don't know what is the benefit of UN ...</td>\n",
       "      <td>0</td>\n",
       "      <td>F25qrkboV3o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18753</th>\n",
       "      <td>Allah Fuckbar</td>\n",
       "      <td>How can I donate to these Buddhists?</td>\n",
       "      <td>593</td>\n",
       "      <td>dZE2nbhR3hU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18754</th>\n",
       "      <td>counter intelligence</td>\n",
       "      <td>fake bikhu</td>\n",
       "      <td>15</td>\n",
       "      <td>dZE2nbhR3hU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18755</th>\n",
       "      <td>Janah Manja</td>\n",
       "      <td>Go To Hell All Men of budha...</td>\n",
       "      <td>63</td>\n",
       "      <td>dZE2nbhR3hU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18756</th>\n",
       "      <td>jamie james</td>\n",
       "      <td>This is so wrong. I feel sorry for the Myanmar...</td>\n",
       "      <td>167</td>\n",
       "      <td>dZE2nbhR3hU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18757</th>\n",
       "      <td>Labir Singh</td>\n",
       "      <td>Why muslims not other races.</td>\n",
       "      <td>76</td>\n",
       "      <td>dZE2nbhR3hU</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18758 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Username  \\\n",
       "0            CuriousLearner   \n",
       "1                   I am me   \n",
       "2                   I am me   \n",
       "3                        OB   \n",
       "4              Khadar Osman   \n",
       "...                     ...   \n",
       "18753         Allah Fuckbar   \n",
       "18754  counter intelligence   \n",
       "18755           Janah Manja   \n",
       "18756           jamie james   \n",
       "18757           Labir Singh   \n",
       "\n",
       "                                                 Comment  Likes     Video ID  \n",
       "0      What about Kashmiri Hindus. Never such mercy f...      0  F25qrkboV3o  \n",
       "1                     10 crore illigal Rohingya in india      0  F25qrkboV3o  \n",
       "2                     10 crore Rohingya muslims in india      1  F25qrkboV3o  \n",
       "3      What happened in Bangladesh before 1980 to cau...      0  F25qrkboV3o  \n",
       "4      I really don't know what is the benefit of UN ...      0  F25qrkboV3o  \n",
       "...                                                  ...    ...          ...  \n",
       "18753               How can I donate to these Buddhists?    593  dZE2nbhR3hU  \n",
       "18754                                         fake bikhu     15  dZE2nbhR3hU  \n",
       "18755                     Go To Hell All Men of budha...     63  dZE2nbhR3hU  \n",
       "18756  This is so wrong. I feel sorry for the Myanmar...    167  dZE2nbhR3hU  \n",
       "18757                       Why muslims not other races.     76  dZE2nbhR3hU  \n",
       "\n",
       "[18758 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../englishComments.csv\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text filtration\n",
    "def filteredTokens(line):\n",
    "\n",
    "    # remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(line)\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "\n",
    "    punctuations = re.compile(r'[-,*%#&\"\\'`:+.?!;()|0-9]')    # remove punctuations\n",
    "    post_punctuations = []\n",
    "    for words in filtered_sentence:\n",
    "        word = punctuations.sub(\"\",words)\n",
    "        if len(word)>0:\n",
    "            post_punctuations.append(word)\n",
    "    \n",
    "    return post_punctuations\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program for KMP Algorithm (works for both string and list) gfg\n",
    "def KMPSearch(pat, txt):\n",
    "\tM = len(pat)\n",
    "\tN = len(txt)\n",
    "\toutput = []\n",
    "\n",
    "\t# create lps[] that will hold the longest prefix suffix\n",
    "\t# values for pattern\n",
    "\tlps = [0]*M\n",
    "\tj = 0 # index for pat[]\n",
    "\n",
    "\t# Preprocess the pattern (calculate lps[] array)\n",
    "\tcomputeLPSArray(pat, M, lps)\n",
    "\n",
    "\ti = 0 # index for txt[]\n",
    "\twhile i < N:\n",
    "\t\tif pat[j] == txt[i]:\n",
    "\t\t\ti += 1\n",
    "\t\t\tj += 1\n",
    "\n",
    "\t\tif j == M:\n",
    "\t\t\toutput.append(i-j)\n",
    "\t\t\tj = lps[j-1]\n",
    "\n",
    "\t\t# mismatch after j matches\n",
    "\t\telif i < N and pat[j] != txt[i]:\n",
    "\t\t\t# Do not match lps[0..lps[j-1]] characters,\n",
    "\t\t\t# they will match anyway\n",
    "\t\t\tif j != 0:\n",
    "\t\t\t\tj = lps[j-1]\n",
    "\t\t\telse:\n",
    "\t\t\t\ti += 1\n",
    "\t\n",
    "\treturn output\n",
    "\n",
    "def computeLPSArray(pat, M, lps):\n",
    "\tlen = 0 # length of the previous longest prefix suffix\n",
    "\n",
    "\tlps[0] # lps[0] is always 0\n",
    "\ti = 1\n",
    "\n",
    "\t# the loop calculates lps[i] for i = 1 to M-1\n",
    "\twhile i < M:\n",
    "\t\tif pat[i] == pat[len]:\n",
    "\t\t\tlen += 1\n",
    "\t\t\tlps[i] = len\n",
    "\t\t\ti += 1\n",
    "\t\telse:\n",
    "\t\t\t# This is tricky. Consider the example.\n",
    "\t\t\t# AAACAAAA and i = 7. The idea is similar\n",
    "\t\t\t# to search step.\n",
    "\t\t\tif len != 0:\n",
    "\t\t\t\tlen = lps[len-1]\n",
    "\n",
    "\t\t\t\t# Also, note that we do not increment i here\n",
    "\t\t\telse:\n",
    "\t\t\t\tlps[i] = 0\n",
    "\t\t\t\ti += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds all occurances of next word after target (string) in source (list)\n",
    "def nextWordOfPhrase(target, source):\n",
    "    \n",
    "    target = word_tokenize(target)\n",
    "    m = len(target)\n",
    "    n = len(source)\n",
    "\n",
    "    lst = KMPSearch(target,source)      # search all occurances of target\n",
    " \n",
    "    nextWords = []\n",
    "    # all next words\n",
    "    for val in lst:\n",
    "        if(m+val) < n:\n",
    "            nextWords.append(source[m+val])\n",
    "    \n",
    "    return nextWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rohingya are and Rohingya are not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud1 = []\n",
    "cloud2 = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    line = df.Comment[i]\n",
    "    line = str(line)\n",
    "    tokens = filteredTokens(line.lower())       # remove unnecessary words & punc.\n",
    "    p1 = nextWordOfPhrase('rohingya',tokens)\n",
    "    p2 = nextWordOfPhrase('illegal', tokens)\n",
    "    cloud1.extend(p1)\n",
    "    cloud2.extend(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save in csv\n",
    "set1, count1 = np.unique(cloud1, return_counts=True)\n",
    "set2, count2 = np.unique(cloud2, return_counts=True)\n",
    "\n",
    "pd.DataFrame({'set1': set1, 'frequency': count1}).to_csv('perception_set1.csv', index=False)\n",
    "pd.DataFrame({'set2': set2, 'frequency': count2}).to_csv('perception_set2.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
