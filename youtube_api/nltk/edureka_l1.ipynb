{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import nltk\r\n",
    "import os\r\n",
    "import nltk.corpus"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#print(os.listdir(nltk.data.find(\"corpora\")))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#nltk.corpus.gutenberg.fileids()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "hamlet = nltk.corpus.gutenberg.words('shakespeare-hamlet.txt')\r\n",
    "hamlet"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', ...]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "for word in hamlet[:20]:\r\n",
    "    print(word, sep = ' ', end = ' ')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ The Tragedie of Hamlet by William Shakespeare 1599 ] Actus Primus . Scoena Prima . Enter Barnardo and Francisco "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "AI = \"week ago a friend invited a couple of other couples over for dinner.\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from nltk.tokenize import word_tokenize"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "AI_tokens = word_tokenize(AI)\r\n",
    "print(AI_tokens)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['week', 'ago', 'a', 'friend', 'invited', 'a', 'couple', 'of', 'other', 'couples', 'over', 'for', 'dinner', '.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from nltk.probability import FreqDist\r\n",
    "fdist = FreqDist()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "for word in AI_tokens:\r\n",
    "    fdist[word.lower()]+=1\r\n",
    "\r\n",
    "fdist"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "FreqDist({'a': 2, 'week': 1, 'ago': 1, 'friend': 1, 'invited': 1, 'couple': 1, 'of': 1, 'other': 1, 'couples': 1, 'over': 1, ...})"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from nltk.tokenize import blankline_tokenize\r\n",
    "AI_blank = blankline_tokenize(AI)\r\n",
    "len(AI_blank)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "from nltk.util import bigrams, trigrams, ngrams\r\n",
    "quotes_tokens = nltk.word_tokenize(AI)\r\n",
    "print(quotes_tokens)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['week', 'ago', 'a', 'friend', 'invited', 'a', 'couple', 'of', 'other', 'couples', 'over', 'for', 'dinner', '.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# bigrams\r\n",
    "q_bigrams = list(nltk.bigrams(quotes_tokens))\r\n",
    "q_bigrams[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('week', 'ago')"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# N-grams\r\n",
    "q_ngrams = list(nltk.ngrams(quotes_tokens,4))\r\n",
    "q_ngrams[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('week', 'ago', 'a', 'friend')"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# Stemming: Normalize word into base form (may not be proper word)\r\n",
    "from nltk.stem import PorterStemmer\r\n",
    "pst = PorterStemmer()\r\n",
    "pst.stem(\"having\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "words_to_stem = [\"give\",\"giving\",\"given\",\"gave\"]\r\n",
    "for words in words_to_stem:\r\n",
    "    print(words+\":\"+pst.stem(words))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "give:give\n",
      "giving:give\n",
      "given:given\n",
      "gave:gave\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "from nltk.stem import LancasterStemmer\r\n",
    "lst = LancasterStemmer()\r\n",
    "for words in words_to_stem:\r\n",
    "    print(words+\":\"+lst.stem(words))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "give:giv\n",
      "giving:giv\n",
      "given:giv\n",
      "gave:gav\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# Lemmatization: Word to its base form (meaningful word)\r\n",
    "from nltk.stem import wordnet\r\n",
    "from nltk.stem import WordNetLemmatizer\r\n",
    "word_lem = WordNetLemmatizer()\r\n",
    "word_lem.lemmatize('corpora')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'corpus'"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# stopwords\r\n",
    "from nltk.corpus import stopwords"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "#stopwords.words('english')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# remove stopwords\r\n",
    "import re\r\n",
    "punctuations = re.compile(r'[-,.?!;()|0-9]')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "post_punctuations = []\r\n",
    "for words in AI_tokens:\r\n",
    "    word = punctuations.sub(\"\",words)\r\n",
    "    if len(word)>0:\r\n",
    "        post_punctuations.append(word)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "print(post_punctuations)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['week', 'ago', 'a', 'friend', 'invited', 'a', 'couple', 'of', 'other', 'couples', 'over', 'for', 'dinner', '']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# pos tagging\r\n",
    "for tokens in AI_tokens:\r\n",
    "    print(nltk.pos_tag([tokens]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('week', 'NN')]\n",
      "[('ago', 'RB')]\n",
      "[('a', 'DT')]\n",
      "[('friend', 'NN')]\n",
      "[('invited', 'VBN')]\n",
      "[('a', 'DT')]\n",
      "[('couple', 'NN')]\n",
      "[('of', 'IN')]\n",
      "[('other', 'JJ')]\n",
      "[('couples', 'NNS')]\n",
      "[('over', 'IN')]\n",
      "[('for', 'IN')]\n",
      "[('dinner', 'NN')]\n",
      "[('.', '.')]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# Named Entity Recognition\r\n",
    "sent = \"All political parties but BJP in Punjab support bandh\"\r\n",
    "from nltk import ne_chunk\r\n",
    "NE_tokens = word_tokenize(sent)\r\n",
    "NE_tags = nltk.pos_tag(NE_tokens)\r\n",
    "NE_NER = ne_chunk(NE_tags)\r\n",
    "print(NE_NER)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(S\n",
      "  All/DT\n",
      "  political/JJ\n",
      "  parties/NNS\n",
      "  but/CC\n",
      "  (ORGANIZATION BJP/NNP)\n",
      "  in/IN\n",
      "  (GPE Punjab/NNP)\n",
      "  support/NN\n",
      "  bandh/NN)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}