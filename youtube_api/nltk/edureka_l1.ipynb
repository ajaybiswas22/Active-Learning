{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94e60b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import nltk.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29dc10af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(os.listdir(nltk.data.find(\"corpora\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "952aaa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21c8cdbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamlet = nltk.corpus.gutenberg.words('shakespeare-hamlet.txt')\n",
    "hamlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "350dd62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ The Tragedie of Hamlet by William Shakespeare 1599 ] Actus Primus . Scoena Prima . Enter Barnardo and Francisco "
     ]
    }
   ],
   "source": [
    "for word in hamlet[:20]:\n",
    "    print(word, sep = ' ', end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a084f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "AI = \"week ago a friend invited a couple of other couples over for dinner.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "685de4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "101b6538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['week', 'ago', 'a', 'friend', 'invited', 'a', 'couple', 'of', 'other', 'couples', 'over', 'for', 'dinner', '.']\n"
     ]
    }
   ],
   "source": [
    "AI_tokens = word_tokenize(AI)\n",
    "print(AI_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "081600e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2593a7fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'a': 2, 'week': 1, 'ago': 1, 'friend': 1, 'invited': 1, 'couple': 1, 'of': 1, 'other': 1, 'couples': 1, 'over': 1, ...})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word in AI_tokens:\n",
    "    fdist[word.lower()]+=1\n",
    "\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d001c137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import blankline_tokenize\n",
    "AI_blank = blankline_tokenize(AI)\n",
    "len(AI_blank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e90368f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['week', 'ago', 'a', 'friend', 'invited', 'a', 'couple', 'of', 'other', 'couples', 'over', 'for', 'dinner', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import bigrams, trigrams, ngrams\n",
    "quotes_tokens = nltk.word_tokenize(AI)\n",
    "print(quotes_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdf55b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('week', 'ago')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bigrams\n",
    "q_bigrams = list(nltk.bigrams(quotes_tokens))\n",
    "q_bigrams[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4806646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('week', 'ago', 'a', 'friend')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N-grams\n",
    "q_ngrams = list(nltk.ngrams(quotes_tokens,4))\n",
    "q_ngrams[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "220d47cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming: Normalize word into base form (may not be proper word)\n",
    "from nltk.stem import PorterStemmer\n",
    "pst = PorterStemmer()\n",
    "pst.stem(\"having\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1b99edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:give\n",
      "giving:give\n",
      "given:given\n",
      "gave:gave\n"
     ]
    }
   ],
   "source": [
    "words_to_stem = [\"give\",\"giving\",\"given\",\"gave\"]\n",
    "for words in words_to_stem:\n",
    "    print(words+\":\"+pst.stem(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65d63667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:giv\n",
      "giving:giv\n",
      "given:giv\n",
      "gave:gav\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lst = LancasterStemmer()\n",
    "for words in words_to_stem:\n",
    "    print(words+\":\"+lst.stem(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "101fcce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'corpus'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatization: Word to its base form (meaningful word)\n",
    "from nltk.stem import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "word_lem = WordNetLemmatizer()\n",
    "word_lem.lemmatize('corpora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f8b1db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "915d2ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c71914db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "import re\n",
    "punctuations = re.compile(r'[-,.?!;()|0-9]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36ca6b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_punctuations = []\n",
    "for words in AI_tokens:\n",
    "    word = punctuations.sub(\"\",words)\n",
    "    if len(words)>0:\n",
    "        post_punctuations.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca6179f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['week', 'ago', 'a', 'friend', 'invited', 'a', 'couple', 'of', 'other', 'couples', 'over', 'for', 'dinner', '']\n"
     ]
    }
   ],
   "source": [
    "print(post_punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6571ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('week', 'NN')]\n",
      "[('ago', 'RB')]\n",
      "[('a', 'DT')]\n",
      "[('friend', 'NN')]\n",
      "[('invited', 'VBN')]\n",
      "[('a', 'DT')]\n",
      "[('couple', 'NN')]\n",
      "[('of', 'IN')]\n",
      "[('other', 'JJ')]\n",
      "[('couples', 'NNS')]\n",
      "[('over', 'IN')]\n",
      "[('for', 'IN')]\n",
      "[('dinner', 'NN')]\n",
      "[('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# pos tagging\n",
    "for tokens in AI_tokens:\n",
    "    print(nltk.pos_tag([tokens]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af8d786c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  All/DT\n",
      "  political/JJ\n",
      "  parties/NNS\n",
      "  but/CC\n",
      "  (ORGANIZATION BJP/NNP)\n",
      "  in/IN\n",
      "  (GPE Punjab/NNP)\n",
      "  support/NN\n",
      "  bandh/NN)\n"
     ]
    }
   ],
   "source": [
    "# Named Entity Recognition\n",
    "sent = \"All political parties but BJP in Punjab support bandh\"\n",
    "from nltk import ne_chunk\n",
    "NE_tokens = word_tokenize(sent)\n",
    "NE_tags = nltk.pos_tag(NE_tokens)\n",
    "NE_NER = ne_chunk(NE_tags)\n",
    "print(NE_NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69ba672",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
