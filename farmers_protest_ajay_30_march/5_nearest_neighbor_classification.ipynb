{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\AJAY\n",
      "[nltk_data]     BISWAS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import fasttext\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "from scipy.spatial import distance\n",
    "from resources.basicIO import InputOutput as IO\n",
    "from resources.tokTT import CommentTokenizer\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "#from sklearn.linear_model import LassoLars     \n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastTextTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Convert texts into their mean fastText vectors \"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.stack([np.mean([self.model[w] for w in text.split()], 0) for text in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(small_model,predictor,lines,Y):\n",
    "    classifier = make_pipeline(\n",
    "        FastTextTransformer(model=small_model),\n",
    "        predictor\n",
    "    ).fit(\n",
    "        lines,\n",
    "        Y\n",
    "    )\n",
    "    return classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# load models\n",
    "model_N_2 = fasttext.load_model('models/ft_unsupervised_N_2.bin')\n",
    "model_N_3 = fasttext.load_model('models/ft_unsupervised_N_3.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load seed set and tokenize\n",
    "seed_set = CommentTokenizer.cleaned(\"datasets/seed_set.txt\")\n",
    "# Load seed Labels\n",
    "Y = IO.load_nums(\"datasets/seed_set_labels.txt\")\n",
    "\n",
    "# Load expanded seed set\n",
    "seed_set_expanded_N_2 = CommentTokenizer.cleaned(\"datasets_post/seed_set_expanded_N_2.txt\")\n",
    "Y_N_2 = IO.load_nums(\"datasets_post/seed_set_expanded_labels_N_2.txt\")\n",
    "\n",
    "seed_set_expanded_N_3 = CommentTokenizer.cleaned(\"datasets_post/seed_set_expanded_N_3.txt\")\n",
    "Y_N_3 = IO.load_nums(\"datasets_post/seed_set_expanded_labels_N_3.txt\")\n",
    "\n",
    "# Load expanded seed set\n",
    "batch_set_expanded_N_2 = CommentTokenizer.cleaned(\"datasets_post/batch_N_2.txt\")\n",
    "Y_batch_N_2 = IO.load_nums(\"datasets_post/batch_labels_N_2.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load testing set\n",
    "testing_text = IO.load_csv_col('datasets/random_sample.csv', 'comment')\n",
    "testing_text = testing_text[500:1000]\n",
    "testing_text_labels = IO.load_csv_col('datasets/random_sample.csv', 'label')\n",
    "testing_text_labels = list(map(int, testing_text_labels[500:1000]))\n",
    "\n",
    "training_text = IO.load_csv_col('datasets/random_sample.csv', 'comment')\n",
    "training_text = training_text[0:500]\n",
    "training_text_labels = IO.load_csv_col('datasets/random_sample.csv', 'label')\n",
    "training_text_labels = list(map(int, training_text_labels[0:500]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"vect\", CountVectorizer()),\n",
    "        (\"tfidf\", TfidfTransformer()),\n",
    "        (\"clf\", LogisticRegression()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = pipeline.fit(training_text, training_text_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.652\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(testing_text)\n",
    "accuracy = metrics.accuracy_score(testing_text_labels, y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.8569376 ,  0.17627913, -0.26320988,  0.6195948 ,  0.40598595,\n",
       "         0.06675431, -0.1140122 ,  0.11172426, -0.26477864,  0.7313269 ,\n",
       "         0.26581547,  0.41145974,  0.09487168, -0.442374  , -0.7185457 ,\n",
       "        -0.27267113,  0.43859398,  0.4779304 , -0.14041455,  0.0945349 ,\n",
       "         0.39918244, -0.10063259,  0.20902435,  0.2797713 ,  0.93947506,\n",
       "        -0.09276822,  0.31877443, -0.83058906, -0.8070138 ,  0.24163577,\n",
       "        -0.70087826,  0.45258236, -0.6021862 , -0.03425203, -0.5246288 ,\n",
       "         0.07123923, -0.20783737, -0.1182274 ,  0.00873116, -0.47064364]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform(model, X):\n",
    "    return np.stack([np.mean([model[w] for w in text.split()], 0) for text in X])\n",
    "\n",
    "X = [\"I am in support of modiji he is right\"]\n",
    "transform(model_N_2,X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification\n",
    "LR_ss_N_2 = classify(model_N_2, LogisticRegression(\n",
    "    random_state=1), seed_set, Y)\n",
    "LR_ss_N_3 = classify(model_N_3, LogisticRegression(\n",
    "    random_state=1), seed_set, Y)\n",
    "LR_es_N_2 = classify(model_N_2, LogisticRegression(\n",
    "    random_state=1), seed_set_expanded_N_2, Y_N_2)\n",
    "LR_es_N_3 = classify(model_N_3,LogisticRegression(\n",
    "    random_state=1),seed_set_expanded_N_3, Y_N_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_ss_N_2 = classify(model_N_2, svm.SVC(), seed_set, Y)\n",
    "SVM_ss_N_3 = classify(model_N_3, svm.SVC(), seed_set, Y)\n",
    "SVM_es_N_2 = classify(model_N_2, svm.SVC(), seed_set_expanded_N_2, Y_N_2)\n",
    "SVM_es_N_3 = classify(model_N_3, svm.SVC(), seed_set_expanded_N_3, Y_N_3)\n",
    "SVM_bes_N_2 = classify(model_N_2, svm.SVC(), batch_set_expanded_N_2, Y_batch_N_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD_ss_N_2 = classify(model_N_2, SGDClassifier(\n",
    "    loss=\"hinge\", penalty=\"l1\"), seed_set, Y)\n",
    "SGD_ss_N_3 = classify(model_N_3, SGDClassifier(\n",
    "    loss=\"hinge\", penalty=\"l1\"), seed_set, Y)\n",
    "SGD_es_N_2 = classify(model_N_2, SGDClassifier(\n",
    "    loss=\"hinge\", penalty=\"l1\"), seed_set_expanded_N_2, Y_N_2)\n",
    "SGD_es_N_3 = classify(model_N_3, SGDClassifier(\n",
    "    loss=\"hinge\", penalty=\"l1\"), seed_set_expanded_N_3, Y_N_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_ss_N_2 = classify(model_N_2, LinearDiscriminantAnalysis(), seed_set, Y)\n",
    "LDA_ss_N_3 = classify(model_N_3, LinearDiscriminantAnalysis(), seed_set, Y)\n",
    "LDA_es_N_2 = classify(model_N_2, LinearDiscriminantAnalysis(),seed_set_expanded_N_2, Y_N_2)\n",
    "LDA_es_N_3 = classify(model_N_3, LinearDiscriminantAnalysis(),seed_set_expanded_N_3, Y_N_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_ss_N_2 = classify(model_N_2, DecisionTreeClassifier(), seed_set, Y)\n",
    "DT_ss_N_3 = classify(model_N_3, DecisionTreeClassifier(), seed_set, Y)\n",
    "DT_es_N_2 = classify(model_N_2, DecisionTreeClassifier(),\n",
    "                      seed_set_expanded_N_2, Y_N_2)\n",
    "DT_es_N_3 = classify(model_N_3, DecisionTreeClassifier(),seed_set_expanded_N_3, Y_N_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "GNB_ss_N_2 = classify(model_N_2, GaussianNB(), seed_set, Y)\n",
    "GNB_ss_N_3 = classify(model_N_3, GaussianNB(), seed_set, Y)\n",
    "GNB_es_N_2 = classify(model_N_2, GaussianNB(),\n",
    "                      seed_set_expanded_N_2, Y_N_2)\n",
    "GNB_es_N_3 = classify(model_N_3, GaussianNB(),seed_set_expanded_N_3, Y_N_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_ss_N_2 = classify(model_N_2, RandomForestClassifier(\n",
    "    n_estimators=50, random_state=1), seed_set, Y)\n",
    "RF_ss_N_3 = classify(model_N_3, RandomForestClassifier(\n",
    "    n_estimators=50, random_state=1), seed_set, Y)\n",
    "RF_es_N_2 = classify(model_N_2, RandomForestClassifier(\n",
    "    n_estimators=50, random_state=1),seed_set_expanded_N_2, Y_N_2)\n",
    "RF_es_N_3 = classify(model_N_3, RandomForestClassifier(\n",
    "    n_estimators=50, random_state=1), seed_set_expanded_N_3, Y_N_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append(('Gaussian NB Seed Set N=2', GNB_ss_N_2))\n",
    "models.append(('Gaussian NB Seed Set N=3', GNB_ss_N_3))\n",
    "models.append(('Gaussian NB Expanded Set N=2', GNB_es_N_2))\n",
    "models.append(('Gaussian NB Expanded Set N=3', GNB_es_N_3))\n",
    "models.append(('LR Seed Set N=2', LR_ss_N_2))\n",
    "models.append(('LR Seed Set N=3', LR_ss_N_3))\n",
    "models.append(('LR Expanded Set N=2', LR_es_N_2))\n",
    "models.append(('LR Expanded Set N=3', LR_es_N_3))\n",
    "models.append(('SVM Seed Set N=2', SVM_ss_N_2))\n",
    "models.append(('SVM Seed Set N=3', SVM_ss_N_3))\n",
    "models.append(('SVM Expanded Set N=2', SVM_es_N_2))\n",
    "models.append(('SVM Expanded Set N=3', SVM_es_N_3))\n",
    "models.append(('SVM Batch Set N=2', SVM_bes_N_2))\n",
    "models.append(('SGD Seed Set N=2', SGD_ss_N_2))\n",
    "models.append(('SGD Seed Set N=3', SGD_ss_N_3))\n",
    "models.append(('SGD Expanded Set N=2', SGD_es_N_2))\n",
    "models.append(('SGD Expanded Set N=3', SGD_es_N_3))\n",
    "models.append(('LDA Seed Set N=2', LDA_ss_N_2))\n",
    "models.append(('LDA Seed Set N=3', LDA_ss_N_3))\n",
    "models.append(('LDA Expanded Set N=2', LDA_es_N_2))\n",
    "models.append(('LDA Expanded Set N=3', LDA_es_N_3))\n",
    "models.append(('Decision Trees Seed Set N=2', DT_ss_N_2))\n",
    "models.append(('Decision Trees Seed Set N=3', DT_ss_N_3))\n",
    "models.append(('Decision Trees Expanded Set N=2', DT_es_N_2))\n",
    "models.append(('Decision Trees Expanded Set N=3', DT_es_N_3))\n",
    "models.append(('Random Forest Seed Set N=2', RF_ss_N_2))\n",
    "models.append(('Random Forest Seed Set N=3', RF_ss_N_3))\n",
    "models.append(('Random Forest Expanded Set N=2', RF_es_N_2))\n",
    "models.append(('Random Forest Expanded Set N=3', RF_es_N_3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print output to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian NB Seed Set N=2\n",
      "Gaussian NB Seed Set N=3\n",
      "Gaussian NB Expanded Set N=2\n",
      "Gaussian NB Expanded Set N=3\n",
      "LR Seed Set N=2\n",
      "LR Seed Set N=3\n",
      "LR Expanded Set N=2\n",
      "LR Expanded Set N=3\n",
      "SVM Seed Set N=2\n",
      "SVM Seed Set N=3\n",
      "SVM Expanded Set N=2\n",
      "SVM Expanded Set N=3\n",
      "SVM Batch Set N=2\n",
      "SGD Seed Set N=2\n",
      "SGD Seed Set N=3\n",
      "SGD Expanded Set N=2\n",
      "SGD Expanded Set N=3\n",
      "LDA Seed Set N=2\n",
      "LDA Seed Set N=3\n",
      "LDA Expanded Set N=2\n",
      "LDA Expanded Set N=3\n",
      "Decision Trees Seed Set N=2\n",
      "Decision Trees Seed Set N=3\n",
      "Decision Trees Expanded Set N=2\n",
      "Decision Trees Expanded Set N=3\n",
      "Random Forest Seed Set N=2\n",
      "Random Forest Seed Set N=3\n",
      "Random Forest Expanded Set N=2\n",
      "Random Forest Expanded Set N=3\n"
     ]
    }
   ],
   "source": [
    "file = open('results/output.txt', 'w+')\n",
    "file.close()\n",
    "\n",
    "outfile = open(\"results/output.txt\", \"a\")\n",
    "for i, v in models:\n",
    "    print(i)\n",
    "    accuracy = metrics.accuracy_score(testing_text_labels, v.predict(testing_text))\n",
    "    confusion_matrix = metrics.confusion_matrix(testing_text_labels, v.predict(testing_text))\n",
    "    print('========= {} Model Test Results ==========='.format(i), file=outfile) \n",
    "    print(' ',file=outfile)\n",
    "    print(\"Model Accuracy:\" \"\\n\", accuracy, file=outfile)\n",
    "    print(' ', file=outfile)\n",
    "    print(\"Confusion matrix:\" \"\\n\", confusion_matrix, file=outfile)\n",
    "    print(' ', file=outfile)\n",
    "outfile.close()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee5cc6fef2d70a7e71ee3826687cbd150f18158e0b1eef11d4f4f92bb920e304"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
