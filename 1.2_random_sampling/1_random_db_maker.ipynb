{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\AJAY\n",
      "[nltk_data]     BISWAS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\AJAY\n",
      "[nltk_data]     BISWAS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "import nltk.corpus\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import bigrams, trigrams, ngrams\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text filtration\n",
    "def filteredTokens(line):\n",
    "\n",
    "    # remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(line)\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "\n",
    "    punctuations = re.compile(\n",
    "        r'[-,*%#&\"\\'`:+.?!;()|0-9]')    # remove punctuations\n",
    "    post_punctuations = []\n",
    "    for words in filtered_sentence:\n",
    "        word = punctuations.sub(\"\", words)\n",
    "        if len(word) > 0:\n",
    "            post_punctuations.append(word)\n",
    "\n",
    "    return post_punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13130, 8459, 2820, 10633, 5632, 2745, 234, 14836, 2009, 5782, 12635, 10836, 3659, 5609, 8290, 5406, 11055, 4224, 1881, 10584, 9959, 2169, 7337, 7367, 2064, 9706, 9968, 14562, 14071, 1157, 12233, 5776, 6182, 6442, 14066, 14670, 2720, 656, 8323, 10656, 58, 8391, 8808, 7399, 4843, 14230, 5521, 3740, 13126, 8686, 2805, 11357, 13953, 6888, 14122, 1367, 5200, 6331, 547, 5694, 14708, 11837, 12040, 11046, 681, 12427, 8471, 10198, 394, 1805, 8399, 14190, 9667, 6102, 11457, 2185, 14643, 7172, 13526, 2028, 1968, 11129, 11490, 148, 8825, 13288, 7313, 2500, 13708, 8540, 2621, 9674, 12317, 11652, 2702, 2107, 11013, 11833, 13891, 3471, 165, 14861, 6481, 12885, 12399, 14261, 7329, 12629, 528, 7520, 12054, 13497, 7155, 1196, 5313, 13885, 7281, 3782, 10711, 7354, 6275, 2262, 9738, 3516, 14228, 11127, 2929, 6854, 12310, 680, 5808, 13620, 12791, 12166, 10768, 10736, 10341, 5924, 13195, 3969, 8044, 12044, 3569, 14858, 4558, 7838, 10994, 252, 10744, 1939, 11414, 6999, 14201, 1688, 9715, 1333, 9083, 537, 14255, 3166, 2978, 10018, 2465, 8631, 6153, 2322, 3925, 10620, 7933, 1086, 4649, 3206, 12308, 7721, 12124, 13675, 7074, 2350, 9659, 9825, 12333, 14971, 2524, 1802, 7900, 10168, 4069, 1794, 10933, 8734, 5240, 11558, 2202, 7906, 9236, 14642, 3493, 1920, 2123, 4755, 13615, 7201, 8095, 12905, 2585, 4525, 5123, 13035, 10862, 5006, 11220, 10098, 3300, 6188, 8969, 2679, 15063, 11533, 14366, 11735, 9008, 11195, 11594, 10510, 6692, 9128, 2407, 12660, 1192, 14771, 5563, 5622, 5989, 11446, 7025, 8691, 5910, 4766, 3440, 9794, 3510, 11133, 6493, 1753, 4845, 1136, 977, 8112, 4282, 3634, 13555, 6020, 11672, 13383, 9319, 9201, 2003, 11102, 7594, 14337, 2646, 6742, 4482, 12959, 3822, 627, 8542, 7054, 4140, 10025, 6966, 9193, 3302, 8980, 5459, 8302, 9848, 10751, 2966, 6935, 13303, 5547, 10236, 265, 7935, 1729, 4496, 9267, 169, 11368, 6832, 12316, 2757, 9118, 524, 14649, 13232, 531, 5698, 5184, 9876, 6097, 7697, 6477, 10362, 3350, 14250, 4084, 14183, 12384, 5902, 12603, 13931, 2137, 13123, 13145, 14151, 11850, 5122, 1068, 13367, 1842, 14758, 3112, 9887, 11400, 10856, 5890, 6458, 12694, 5049, 3648, 14711, 11874, 430, 13637, 4307, 8790, 8971, 10992, 13325, 546, 3456, 3988, 8616, 11387, 3796, 7543, 1592, 99, 10055, 10931, 9804, 6035, 12612, 9060, 3891, 5908, 12083, 11307, 10139, 10293, 5433, 7424, 6817, 3610, 2761, 6456, 6432, 10024, 12131, 2728, 7226, 12019, 4880, 271, 14696, 7005, 12405, 12728, 7328, 14860, 1137, 490, 3665, 11176, 3187, 14419, 3820, 4591, 10928, 6716, 7212, 10500, 11485, 13668, 9639, 607, 6466, 2913, 10092, 12598, 11835, 806, 2377, 6437, 1622, 8656, 5965, 11894, 9175, 8366, 12491, 7563, 12505, 1921, 2990, 5893, 5188, 892, 3361, 1050, 4414, 14753, 10873, 10525, 12597, 8780, 3520, 9467, 135, 4620, 1916, 4415, 14547, 9156, 14466, 13459, 1843, 7374, 14631, 7179, 6396, 9525, 1252, 14869, 12103, 5011, 8858, 5324, 3307, 6309, 5537, 8306, 9961, 1521, 14954, 11235, 13180, 973, 7913, 13887, 14857, 13829, 8907, 14287, 7468, 3195, 11792, 14087, 3661, 11422, 10588, 2599, 8169, 5752, 10456, 386, 8720, 11315, 5335, 8101, 11395, 14220, 2275, 11289, 943, 7552, 10089, 5691, 12934, 3975, 3376, 9558, 8248, 12235, 13427, 491, 8731, 10363, 7854, 13581, 10396, 11885, 4465, 8473, 6467, 14551, 8408, 2240, 12053, 14326, 12907, 14840, 4280, 5835, 6666, 7204, 11312, 2259, 11068, 14610, 3819, 4013, 10538, 3189, 2446, 5126, 10554, 585, 13893, 1492, 7629, 3774, 2400, 2808, 5548, 12513, 6790, 15048, 1343, 12130, 3494, 1613, 7957, 4582, 13014, 8728, 926, 10785, 6588, 10021, 8934, 13803, 12218, 5594, 7471, 7053, 9341, 1460, 12634, 10277, 8622, 2620, 11443, 2992, 6459, 14383, 12299, 10095, 8610, 6829, 14746, 5922, 7972, 8964, 2197, 671, 1650, 1684, 7114, 5349, 8899, 11000, 8793, 5837, 7086, 9134, 7122, 1956, 1200, 13970, 14885, 9009, 8453, 67, 12757, 11867, 14385, 1624]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"englishComments.csv\")\n",
    "\n",
    "rlist = random.sample(range(0, len(df)), 600)\n",
    "print(rlist)\n",
    "\n",
    "comments = []\n",
    "\n",
    "for i in range(len(rlist)):\n",
    "    line = df.Comment[i]\n",
    "    line = str(line)\n",
    "    # remove unnecessary words & punc.\n",
    "    #tokens = filteredTokens(line.lower())\n",
    "    comments.append(line)\n",
    "\n",
    "pd.DataFrame({'comment': comments}).to_csv('random_sample.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee5cc6fef2d70a7e71ee3826687cbd150f18158e0b1eef11d4f4f92bb920e304"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
